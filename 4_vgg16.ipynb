{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# pip install torchsummary\n",
    "# https://deepbaksuvision.github.io/Modu_ObjectDetection/posts/03_04_torchsummary.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequential로 묶어서 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. sequential 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "       \n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "          \n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(9*9*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.layer(x)\n",
    "        print(out.shape)\n",
    "#         out=out.view(batch_size, -1)\n",
    "        out=out.view(out.size(0), -1)\n",
    "        out=self.fc_layer(out)\n",
    "        print(\"최종 shape:\", out.shape)\n",
    "        return out\n",
    "          \n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image: torch.Size([1, 3, 300, 300])\n",
      "torch.Size([1, 512, 9, 9])\n",
      "최종 shape: torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1667e-02, -9.0218e-04,  1.8828e-02, -8.7136e-04, -8.8349e-03,\n",
       "         -5.5008e-03,  1.0682e-02,  7.7926e-03,  9.9613e-03, -7.2921e-03,\n",
       "         -1.1382e-02, -1.3517e-02,  4.3119e-03, -5.8750e-04, -3.9414e-03,\n",
       "          2.4665e-03, -7.9875e-03, -8.6256e-04, -1.0061e-02, -1.5009e-02,\n",
       "          5.1601e-03,  1.3538e-02, -4.6324e-03, -7.4240e-03,  4.8207e-03,\n",
       "          1.7910e-02,  1.2387e-02, -7.0516e-03,  1.0797e-02,  1.0968e-02,\n",
       "          8.7332e-03,  8.4310e-03,  1.6224e-03,  5.9640e-03, -4.9594e-03,\n",
       "          1.7580e-03, -6.2760e-03,  5.7726e-03, -1.0735e-02, -2.2033e-03,\n",
       "         -8.2752e-03, -1.9055e-03, -1.4209e-02,  8.2118e-03, -1.5375e-02,\n",
       "         -3.8090e-03,  1.2895e-02,  1.2538e-02, -6.3928e-03, -1.4047e-02,\n",
       "          1.0555e-02, -1.7609e-04,  4.3523e-03, -9.4448e-03,  6.1869e-03,\n",
       "          2.0335e-03,  1.0966e-02,  4.1934e-03,  5.1101e-03, -7.7356e-03,\n",
       "         -1.1596e-02,  1.3500e-02, -7.0593e-03, -6.4340e-03,  1.2341e-02,\n",
       "         -2.8317e-03,  8.6287e-03,  7.5007e-03,  1.0347e-02, -5.9873e-03,\n",
       "          1.5688e-02, -8.0903e-03, -6.7880e-03,  8.6133e-03, -4.2194e-03,\n",
       "         -7.6704e-03, -5.6740e-03,  5.4583e-03, -8.4575e-03, -6.1710e-03,\n",
       "          6.6429e-03, -3.2518e-03,  1.4701e-02, -1.2471e-03,  4.1803e-03,\n",
       "         -3.8981e-03,  4.5070e-04,  1.4545e-02, -1.2062e-02, -2.4270e-02,\n",
       "          3.5015e-03,  1.1984e-02, -1.3358e-02, -1.0104e-02,  1.4888e-02,\n",
       "          8.1908e-03, -1.0393e-02, -1.4086e-02,  4.3208e-03,  5.2671e-04,\n",
       "         -4.2477e-03,  5.7963e-03,  8.1084e-03,  1.4501e-02, -6.6524e-03,\n",
       "         -1.3340e-02,  1.2031e-02, -9.7855e-03, -4.5610e-03,  1.1850e-02,\n",
       "          1.7380e-02,  1.1125e-02,  1.1638e-02,  3.3441e-03, -2.3012e-03,\n",
       "          1.3265e-02,  5.5414e-03,  2.7598e-03, -9.4369e-03,  1.5154e-02,\n",
       "          1.1822e-02, -1.9734e-02,  9.1219e-03, -3.1113e-03,  7.4841e-03,\n",
       "         -3.1573e-03,  1.7803e-04, -9.6622e-03, -3.3117e-03, -1.1602e-02,\n",
       "          1.1027e-02,  1.6388e-03,  4.8887e-03, -1.8943e-02,  9.9976e-03,\n",
       "         -1.4834e-02, -1.3648e-02, -1.0579e-02, -1.6451e-02, -4.1013e-03,\n",
       "          6.4759e-03, -7.7350e-03,  9.2734e-03, -1.0649e-02,  3.2126e-03,\n",
       "         -2.7570e-03, -1.3013e-02,  7.3524e-03, -7.4312e-03, -1.7877e-02,\n",
       "          1.2980e-02, -2.3198e-03, -2.9819e-03, -9.9958e-04, -1.5906e-03,\n",
       "         -9.1981e-03,  9.1506e-03, -3.7013e-03,  5.7089e-03,  3.2835e-03,\n",
       "          5.6429e-03,  2.5792e-03, -1.2056e-02,  1.5176e-02,  1.5137e-02,\n",
       "         -2.3598e-04,  5.7815e-03, -1.5208e-02, -8.5982e-03,  5.3679e-03,\n",
       "         -1.0080e-02,  8.2650e-03, -1.7067e-02,  7.3946e-03,  2.7264e-04,\n",
       "          6.7921e-03, -7.6047e-03, -3.9820e-03, -5.6615e-03, -7.2064e-03,\n",
       "          1.3147e-02, -6.9664e-03,  1.1644e-03,  1.6859e-02, -4.4046e-04,\n",
       "          1.0683e-02,  1.2277e-02, -2.0172e-03, -1.1186e-02, -1.1261e-02,\n",
       "          1.1420e-02,  1.4424e-03, -6.1265e-03, -1.0241e-02, -1.0656e-03,\n",
       "         -5.9687e-03,  5.3386e-03, -4.0669e-04, -7.6000e-04,  9.5102e-03,\n",
       "         -1.8393e-02,  1.7202e-02, -7.4161e-03,  1.9421e-03, -6.3401e-03,\n",
       "         -1.6136e-02,  7.2880e-03,  1.0648e-02,  5.6880e-03,  8.3265e-03,\n",
       "          5.5153e-03,  2.4663e-03,  2.1407e-03,  6.5900e-03,  5.8647e-03,\n",
       "          9.8983e-03,  4.3656e-03, -1.3412e-02,  3.1447e-03, -4.3630e-04,\n",
       "         -1.2342e-02, -5.5191e-03,  7.3948e-03, -5.8627e-03, -4.6526e-03,\n",
       "          1.1235e-02, -1.1393e-02,  1.4116e-02,  1.4077e-02,  1.2568e-03,\n",
       "          1.1270e-02,  7.4654e-03, -4.8882e-03, -4.2126e-04,  2.0260e-03,\n",
       "         -1.7174e-03,  1.8477e-02, -2.1281e-03,  1.5079e-02, -1.6568e-02,\n",
       "         -5.0796e-03,  2.9142e-03,  1.0878e-02, -8.8698e-03, -2.0961e-04,\n",
       "          8.2649e-03,  1.2040e-02,  1.5621e-02, -8.5176e-03, -8.3771e-03,\n",
       "         -4.2709e-03, -1.3883e-02, -3.4618e-03, -3.1871e-03, -1.1245e-02,\n",
       "          9.1919e-03,  1.3458e-02, -6.3669e-03, -1.3091e-03,  1.1388e-03,\n",
       "          2.9526e-03, -1.4657e-02,  2.3226e-03, -5.5459e-03,  1.3920e-02,\n",
       "         -1.2959e-02, -7.3606e-03, -1.1643e-02, -5.8586e-03,  8.8777e-03,\n",
       "         -3.1541e-03,  1.0495e-03,  6.1112e-03, -4.7938e-04, -1.4303e-02,\n",
       "         -1.6145e-02, -1.0725e-02, -3.6062e-03, -5.3143e-03, -9.8759e-03,\n",
       "         -1.1340e-02,  6.5982e-03,  2.5546e-03, -1.2990e-02, -1.5306e-02,\n",
       "          9.1353e-03,  9.7370e-03, -2.0876e-02,  6.8698e-03, -1.2332e-02,\n",
       "          7.9280e-03, -4.6094e-03,  4.5161e-03,  6.6762e-04, -3.9544e-03,\n",
       "          6.1919e-03, -7.5596e-03, -1.0554e-02, -4.5598e-03, -7.3095e-03,\n",
       "         -1.3764e-02,  8.5074e-03, -4.3152e-03, -1.8766e-02, -1.2790e-02,\n",
       "          2.1769e-03, -1.3619e-02, -8.9373e-03,  5.8514e-03,  2.2742e-03,\n",
       "         -1.8700e-02, -1.2278e-02, -3.3895e-03, -7.5468e-03,  4.1575e-03,\n",
       "          1.8315e-02, -1.5303e-02, -8.2491e-03, -2.5586e-03, -1.5185e-02,\n",
       "         -1.4910e-02, -1.2601e-02, -1.4676e-02, -7.5762e-04, -6.6724e-03,\n",
       "         -1.0971e-02,  1.1286e-02,  3.3169e-03, -1.5238e-02,  5.7323e-03,\n",
       "         -4.1510e-03,  1.5791e-02, -1.2230e-02, -2.4862e-03, -1.0141e-02,\n",
       "          3.7680e-03,  5.6310e-03, -1.7312e-02, -1.0326e-02,  5.9895e-03,\n",
       "          2.1445e-02,  6.9401e-03, -1.2815e-02, -1.3831e-02,  1.5838e-02,\n",
       "          8.7035e-03,  5.2079e-03,  3.7159e-03, -1.0401e-02, -2.1449e-03,\n",
       "         -7.3567e-03,  1.6065e-02,  8.6982e-03, -4.1012e-03,  1.1368e-02,\n",
       "         -1.4659e-02, -1.9779e-03, -7.3864e-03, -1.2307e-02,  5.6880e-03,\n",
       "          1.5236e-02,  4.9389e-03,  4.2515e-03,  1.3780e-02, -1.8279e-02,\n",
       "         -8.1481e-03, -6.9997e-03,  1.8788e-02,  8.3890e-03, -1.3890e-02,\n",
       "          8.3556e-03,  3.9509e-03,  6.1126e-03,  4.3203e-03, -1.8708e-02,\n",
       "         -1.1969e-02, -8.2183e-03, -1.3461e-02,  6.8639e-03,  1.3990e-02,\n",
       "         -4.5837e-03,  1.9022e-02,  1.1295e-02,  8.5301e-03,  1.3413e-03,\n",
       "         -1.3107e-02,  1.2139e-02, -4.2134e-03,  9.3757e-03,  7.0975e-03,\n",
       "          6.6021e-03, -1.3601e-02,  5.6893e-03, -1.3621e-02, -4.8733e-03,\n",
       "          5.8849e-03, -1.3145e-02, -1.0468e-02, -3.2884e-03,  9.2485e-04,\n",
       "          1.7717e-02,  1.5994e-02, -1.8564e-03, -1.0831e-02,  8.1952e-03,\n",
       "          1.0534e-02,  2.4992e-03,  1.8683e-02, -8.4553e-03, -2.1219e-03,\n",
       "         -8.8864e-03,  1.3806e-02,  4.6924e-03, -7.4046e-03, -4.6738e-04,\n",
       "         -1.0273e-02,  1.2795e-02,  1.1926e-02,  8.1519e-03,  1.0894e-03,\n",
       "         -2.2385e-02,  3.4541e-03, -7.1710e-03, -3.8305e-03, -1.5785e-03,\n",
       "          1.5976e-02, -8.1887e-03, -1.3960e-02, -2.4074e-02, -5.9544e-03,\n",
       "          5.0724e-03, -4.0958e-03, -1.3002e-02, -2.9801e-03,  8.1105e-03,\n",
       "         -4.1243e-03, -2.6104e-03,  2.0993e-03,  5.4328e-03,  1.3519e-02,\n",
       "          1.0934e-02,  1.2449e-02,  7.9916e-03,  9.5292e-03,  7.0187e-03,\n",
       "          7.6070e-03, -4.5668e-03, -3.6820e-03, -7.4189e-03,  1.3664e-02,\n",
       "         -5.2458e-03,  6.0446e-03,  1.2805e-02,  9.3762e-03,  1.1056e-02,\n",
       "         -3.6500e-03,  8.0132e-03, -2.7074e-03,  6.4965e-03, -4.0518e-03,\n",
       "          5.8030e-03,  4.1707e-03, -4.6857e-03, -1.0484e-02,  1.1276e-02,\n",
       "          3.9210e-03,  8.3218e-03,  2.3949e-03, -2.1978e-03,  1.3692e-02,\n",
       "          6.9737e-03, -1.4564e-02,  2.3970e-03,  1.6736e-02,  6.2395e-03,\n",
       "          1.7715e-02, -2.8549e-03, -7.9531e-03, -1.2738e-03, -5.3661e-03,\n",
       "         -1.7202e-02,  1.9251e-03, -1.0523e-03, -1.3769e-03, -1.3514e-02,\n",
       "         -3.5239e-03, -4.4049e-03,  1.7636e-02, -1.0598e-02,  1.2664e-03,\n",
       "         -9.1902e-03, -4.4996e-03, -6.3504e-03, -3.2459e-03,  1.1084e-02,\n",
       "         -1.6724e-04,  8.1485e-03,  4.3945e-03, -1.0552e-02,  5.8039e-03,\n",
       "         -3.8160e-03, -1.5839e-03, -2.0860e-03, -1.5464e-02,  6.4122e-07,\n",
       "         -3.4611e-03, -1.0227e-02, -4.3955e-03, -6.5674e-04, -7.2156e-03,\n",
       "         -5.2774e-03, -9.6230e-03, -1.2381e-02,  9.9932e-03, -1.0800e-02,\n",
       "          4.7665e-03, -1.3661e-02,  2.7303e-03, -9.0974e-03,  9.2044e-03,\n",
       "         -1.0414e-02,  5.1686e-03,  3.7488e-03,  6.7350e-03, -4.4466e-03,\n",
       "          2.6223e-03,  4.5583e-03,  1.1331e-03,  1.3409e-03, -8.7366e-03,\n",
       "          1.9555e-03,  1.5308e-03, -1.8356e-02, -4.7490e-03, -1.4621e-02,\n",
       "          5.6612e-03,  1.2254e-02, -5.9047e-04, -6.5223e-03,  5.8173e-03,\n",
       "         -1.0393e-02,  1.2817e-02,  8.4364e-03, -9.2717e-03, -9.9460e-03,\n",
       "          2.5441e-03, -1.1213e-02, -1.4274e-02,  1.9313e-02, -3.1773e-03,\n",
       "          1.2172e-02,  1.0993e-02, -3.2772e-03,  1.0759e-02,  5.5498e-03,\n",
       "          4.0445e-03,  1.2096e-03, -1.7535e-02, -1.0987e-02, -6.2266e-03,\n",
       "          1.6982e-02,  1.6591e-04,  8.7818e-03,  1.8048e-02,  7.2010e-03,\n",
       "         -1.3294e-03, -6.1727e-03, -1.1697e-02,  6.6417e-03, -7.8810e-03,\n",
       "         -1.1933e-02, -7.4262e-03, -1.3367e-02, -4.9383e-03, -1.6893e-02,\n",
       "          5.4640e-03, -3.6153e-03, -6.1743e-03,  8.5842e-03,  1.0596e-02,\n",
       "          1.0258e-02, -2.6298e-03,  3.6130e-03, -1.7985e-02,  3.6382e-03,\n",
       "         -4.3937e-03,  1.3563e-02, -8.7964e-03, -1.0047e-02,  4.4057e-03,\n",
       "          2.7914e-03, -7.6721e-03, -2.0344e-02,  1.2422e-02,  1.1720e-02,\n",
       "         -1.3422e-02,  5.2902e-03,  3.2162e-03, -3.0834e-03, -6.0800e-04,\n",
       "          6.5658e-03, -2.8245e-03, -8.2797e-03, -1.3004e-02, -2.3717e-03,\n",
       "         -9.3355e-03, -1.7273e-03, -8.5580e-03,  9.3252e-03,  6.2564e-03,\n",
       "         -6.8932e-03, -4.1480e-03, -1.6640e-02, -1.2036e-03, -1.2214e-02,\n",
       "          9.6566e-03, -3.0486e-03, -5.3166e-03, -1.2552e-02, -1.0309e-02,\n",
       "          1.4418e-02,  4.7526e-03, -9.9750e-04,  5.3136e-03,  8.3207e-05,\n",
       "          1.6093e-03, -6.7992e-03, -1.1289e-03,  1.4481e-02,  7.8033e-03,\n",
       "         -4.2585e-04, -3.6520e-03,  1.4805e-02, -7.5451e-03,  5.7724e-04,\n",
       "          1.4207e-02, -1.4472e-02, -5.9720e-03,  8.7220e-03,  3.6108e-04,\n",
       "          3.8242e-03,  1.3949e-03, -5.4686e-03,  1.2101e-02,  1.2803e-02,\n",
       "          1.6337e-02,  9.6934e-03, -1.7009e-02, -1.1468e-03, -1.1356e-02,\n",
       "          1.5985e-02, -9.7835e-03, -8.1185e-04,  1.4112e-02,  5.0382e-03,\n",
       "         -7.5973e-03,  6.1831e-05,  9.4921e-03, -8.3607e-03,  3.2862e-03,\n",
       "         -6.8143e-03,  9.0036e-03,  3.1287e-03,  4.8086e-03,  3.1885e-03,\n",
       "          2.1905e-02,  8.8375e-03, -9.1244e-03,  8.8254e-03, -1.1294e-02,\n",
       "         -3.4156e-03, -4.3471e-03,  6.5996e-03,  3.7577e-03,  8.1225e-03,\n",
       "         -1.8848e-02,  5.0245e-03,  5.9942e-04,  1.1785e-02,  9.2840e-03,\n",
       "         -3.6081e-03,  1.8443e-02, -1.6897e-02,  1.4199e-03,  1.7897e-02,\n",
       "          1.4101e-02, -4.3309e-03,  1.5209e-02,  1.2101e-02,  4.8217e-03,\n",
       "         -3.9618e-03,  1.4515e-02,  5.2460e-04,  6.3634e-03, -2.0228e-03,\n",
       "          1.1587e-02,  3.6908e-03,  9.7383e-03, -1.2900e-02, -1.1069e-02,\n",
       "         -1.1152e-02, -1.2155e-02,  6.5884e-03,  1.5730e-02, -1.4406e-02,\n",
       "         -2.5219e-03, -1.0536e-02,  1.1697e-02, -8.9287e-03, -4.9067e-03,\n",
       "          8.8912e-03, -1.2916e-02, -9.1963e-04, -1.4161e-02,  1.2258e-02,\n",
       "          4.8094e-03,  9.6314e-03, -1.1315e-02,  1.4809e-02, -2.7583e-03,\n",
       "         -2.5566e-03,  5.2469e-03, -1.5771e-02,  8.6702e-03, -6.4536e-03,\n",
       "         -5.0808e-03, -1.2403e-03,  9.5859e-03, -1.2547e-02,  1.1466e-02,\n",
       "         -1.3171e-02, -1.1125e-02, -1.2297e-02, -5.9669e-03,  1.2921e-02,\n",
       "         -1.2619e-02, -2.4220e-03,  1.1632e-02, -3.4884e-03, -2.3482e-03,\n",
       "         -9.1984e-04,  1.2740e-02, -1.7217e-02, -1.6537e-02, -6.0929e-03,\n",
       "          7.7973e-03, -1.5635e-02,  1.0568e-02, -2.7233e-03, -8.5455e-03,\n",
       "          1.1533e-02,  1.8944e-02,  8.6079e-03,  2.1069e-03,  1.5076e-02,\n",
       "          1.1721e-02, -1.6715e-02,  9.5056e-03,  1.3198e-02, -8.9875e-03,\n",
       "          4.4926e-03,  1.3913e-02, -2.3704e-03, -5.1411e-04,  1.6422e-02,\n",
       "          1.1004e-02,  9.2088e-03, -8.6664e-03,  1.4268e-02, -1.4212e-02,\n",
       "         -5.8584e-03,  1.0416e-03, -1.1035e-02, -1.0636e-02,  1.1879e-02,\n",
       "         -1.9005e-03,  5.7352e-03, -8.7601e-03,  1.0924e-02,  6.9286e-03,\n",
       "          2.3219e-03,  9.8866e-03,  1.7355e-02,  2.3728e-03,  6.8235e-03,\n",
       "          1.4325e-02, -7.9116e-03, -1.1580e-02, -8.1880e-04, -7.7992e-03,\n",
       "          9.9462e-03,  6.7184e-03, -1.2050e-03,  7.1256e-03,  9.4066e-03,\n",
       "         -3.6515e-03, -4.6495e-03, -1.0198e-02,  5.2395e-03,  1.1398e-02,\n",
       "         -1.3275e-02, -2.2606e-03, -2.0693e-02,  9.7010e-03,  8.7059e-03,\n",
       "          9.1965e-03, -1.0298e-02, -8.9321e-05,  1.2906e-03,  8.3448e-03,\n",
       "         -7.7750e-04, -8.6637e-03,  1.0501e-02,  7.1560e-03,  1.2353e-02,\n",
       "         -8.7860e-03, -3.7666e-03, -1.4579e-02, -7.8636e-03,  6.5458e-03,\n",
       "          7.6620e-03, -4.1035e-03, -1.3697e-02,  1.5303e-03, -3.7556e-03,\n",
       "         -6.1033e-04,  9.7976e-03, -1.0826e-02,  3.0749e-03,  1.5635e-05,\n",
       "         -8.5976e-04,  6.5225e-03, -4.2649e-03,  1.3605e-02, -1.3813e-02,\n",
       "          4.6003e-03,  2.0811e-05,  1.3527e-02, -8.0737e-03, -5.3050e-03,\n",
       "          9.9114e-03, -2.0302e-02,  9.2082e-03,  7.7064e-03, -1.1313e-02,\n",
       "          3.7756e-03,  1.0233e-03,  1.7694e-02,  7.9326e-03, -7.2237e-03,\n",
       "          6.6755e-03,  1.5592e-02,  1.3284e-02,  6.7577e-03, -3.1304e-03,\n",
       "         -9.2714e-03, -5.4541e-03, -5.1194e-03, -3.1894e-03,  9.8324e-03,\n",
       "          5.9723e-03,  1.8820e-03,  4.9110e-03, -3.4902e-03, -1.3714e-02,\n",
       "         -3.8347e-03, -4.3365e-03, -6.8036e-03,  8.1363e-03, -1.4200e-02,\n",
       "          1.3620e-02, -8.5739e-03,  1.0925e-03,  5.6627e-03,  1.2824e-02,\n",
       "          1.1982e-02, -9.6554e-03, -1.2005e-02, -7.8680e-03, -1.4483e-02,\n",
       "         -8.1416e-03,  1.5734e-02, -7.7017e-03,  1.0221e-03,  1.2609e-02,\n",
       "          6.3627e-03, -4.7650e-03,  2.5235e-03,  1.5902e-02,  8.1989e-03,\n",
       "          2.4623e-03,  2.9633e-03,  2.9938e-03,  9.8677e-03,  1.3412e-02,\n",
       "          1.0690e-02, -1.3580e-02,  1.3545e-02, -2.1634e-02, -1.6254e-02,\n",
       "         -1.5761e-03,  7.5467e-03, -4.7003e-03,  1.3554e-02, -7.7669e-03,\n",
       "         -1.4072e-02, -1.2294e-02,  1.0792e-02, -8.5935e-03,  3.6399e-03,\n",
       "         -7.3594e-03, -2.6110e-03, -9.0095e-03, -1.4985e-02,  3.6847e-03,\n",
       "          7.3746e-03,  5.3312e-03,  1.0755e-02,  1.6798e-02,  1.3687e-02,\n",
       "          1.4544e-02,  8.4751e-04, -1.0679e-02, -5.7452e-03,  2.5784e-03,\n",
       "         -7.1351e-03,  1.3828e-02, -1.4347e-02,  2.8246e-03,  3.0386e-03,\n",
       "          1.1177e-02,  1.2871e-02, -3.7481e-03, -1.1817e-02, -3.9101e-03,\n",
       "         -1.2186e-02,  1.0917e-02, -1.3127e-02,  2.6324e-03, -1.2776e-02,\n",
       "          1.4239e-02, -1.0326e-02, -8.4697e-03,  3.6014e-03,  7.4317e-03,\n",
       "          9.5351e-03,  2.5758e-03, -4.6508e-03, -4.7984e-03,  1.4094e-02,\n",
       "         -9.3201e-03,  8.8714e-03, -1.0544e-02,  1.4116e-02,  8.9955e-03,\n",
       "          1.1344e-03, -7.5149e-03, -2.6909e-03, -1.5671e-02,  3.2165e-03,\n",
       "         -8.8640e-03,  3.4816e-03, -1.2726e-02,  1.0169e-02,  6.3988e-04,\n",
       "          7.3299e-03,  7.4556e-03,  1.5057e-02,  9.0012e-03,  1.3026e-02,\n",
       "         -5.7455e-03,  4.6267e-03,  7.4244e-03, -1.3974e-02, -9.8161e-03,\n",
       "          5.8910e-03,  9.3629e-03, -1.3565e-02,  3.2185e-03, -4.7005e-03,\n",
       "         -6.7029e-03,  7.6301e-04, -1.4386e-02,  2.1215e-04,  1.0918e-02,\n",
       "         -1.2364e-02, -8.8007e-03, -4.0489e-03,  5.7674e-03, -1.5894e-02,\n",
       "         -1.3503e-02,  1.0752e-03, -1.4833e-02, -2.1954e-03,  1.2090e-02,\n",
       "         -4.9599e-03,  4.0067e-03, -8.5175e-03,  2.1347e-03,  1.5297e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image:\", input_image.shape)\n",
    "\n",
    "vgg = VGG16()\n",
    "vgg.__init__()\n",
    "vgg.forward(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. feature map을 추출할 수 있도록 sequential 3개로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/vgg16.png\" width=\"700\"/>\n",
    "\n",
    "<!-- ![VGG-16](img/vgg16.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_v2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(            \n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(            \n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(9*9*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.layer1(x)\n",
    "        print(\"1st feature map:\", out.shape)\n",
    "        out=self.layer2(out)\n",
    "        print(\"2nd feature map:\", out.shape)\n",
    "        out=self.layer3(out)\n",
    "        print(\"3rd feature map:\", out.shape)\n",
    "        out=out.view(out.size(0), -1)\n",
    "        out=self.fc_layer(out)\n",
    "        print(\"fc layer shape:\", out.shape)\n",
    "        return out\n",
    "        \n",
    "model = VGG16_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st feature map: torch.Size([2, 256, 37, 37])\n",
      "2nd feature map: torch.Size([2, 512, 18, 18])\n",
      "3rd feature map: torch.Size([2, 512, 9, 9])\n",
      "fc layer shape: torch.Size([2, 1000])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 300, 300]           1,792\n",
      "              ReLU-2         [-1, 64, 300, 300]               0\n",
      "            Conv2d-3         [-1, 64, 300, 300]          36,928\n",
      "              ReLU-4         [-1, 64, 300, 300]               0\n",
      "         MaxPool2d-5         [-1, 64, 150, 150]               0\n",
      "              ReLU-6         [-1, 64, 150, 150]               0\n",
      "            Conv2d-7        [-1, 128, 150, 150]          73,856\n",
      "              ReLU-8        [-1, 128, 150, 150]               0\n",
      "            Conv2d-9        [-1, 128, 150, 150]         147,584\n",
      "             ReLU-10        [-1, 128, 150, 150]               0\n",
      "        MaxPool2d-11          [-1, 128, 75, 75]               0\n",
      "             ReLU-12          [-1, 128, 75, 75]               0\n",
      "           Conv2d-13          [-1, 256, 75, 75]         295,168\n",
      "             ReLU-14          [-1, 256, 75, 75]               0\n",
      "           Conv2d-15          [-1, 256, 75, 75]         590,080\n",
      "             ReLU-16          [-1, 256, 75, 75]               0\n",
      "        MaxPool2d-17          [-1, 256, 37, 37]               0\n",
      "             ReLU-18          [-1, 256, 37, 37]               0\n",
      "           Conv2d-19          [-1, 512, 37, 37]       1,180,160\n",
      "             ReLU-20          [-1, 512, 37, 37]               0\n",
      "           Conv2d-21          [-1, 512, 37, 37]       2,359,808\n",
      "             ReLU-22          [-1, 512, 37, 37]               0\n",
      "           Conv2d-23          [-1, 512, 37, 37]       2,359,808\n",
      "             ReLU-24          [-1, 512, 37, 37]               0\n",
      "        MaxPool2d-25          [-1, 512, 18, 18]               0\n",
      "             ReLU-26          [-1, 512, 18, 18]               0\n",
      "           Conv2d-27          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-28          [-1, 512, 18, 18]               0\n",
      "           Conv2d-29          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-30          [-1, 512, 18, 18]               0\n",
      "           Conv2d-31          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-32          [-1, 512, 18, 18]               0\n",
      "        MaxPool2d-33            [-1, 512, 9, 9]               0\n",
      "             ReLU-34            [-1, 512, 9, 9]               0\n",
      "           Linear-35                 [-1, 4096]     169,873,408\n",
      "             ReLU-36                 [-1, 4096]               0\n",
      "           Linear-37                 [-1, 4096]      16,781,312\n",
      "             ReLU-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 204,876,328\n",
      "Trainable params: 204,876,328\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 388.90\n",
      "Params size (MB): 781.54\n",
      "Estimated Total Size (MB): 1171.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image: torch.Size([1, 3, 300, 300])\n",
      "1st feature map: torch.Size([1, 256, 37, 37])\n",
      "2nd feature map: torch.Size([1, 512, 18, 18])\n",
      "3rd feature map: torch.Size([1, 512, 9, 9])\n",
      "fc layer shape: torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6627e-03, -5.4811e-03,  7.1940e-03,  2.8413e-03,  3.5779e-03,\n",
       "          1.2086e-02, -1.0418e-02,  2.4029e-02,  5.7935e-03,  1.0993e-02,\n",
       "          3.3750e-03,  5.5346e-03, -8.7306e-03, -8.7923e-04, -1.3079e-02,\n",
       "          1.1408e-02,  7.0815e-03,  3.3063e-03, -1.8808e-03,  4.0098e-04,\n",
       "          3.7818e-03,  7.5980e-04,  4.6099e-03, -4.4130e-03, -9.7615e-03,\n",
       "         -2.6397e-03,  2.2819e-03, -1.5259e-02, -2.2287e-03,  1.3305e-03,\n",
       "          7.6571e-03,  1.0547e-02,  1.0508e-03, -1.3387e-02,  1.9768e-02,\n",
       "          1.1887e-02, -3.9477e-03, -6.9013e-03, -1.0001e-03,  1.8150e-02,\n",
       "         -8.6054e-03,  1.0927e-02,  7.4750e-03,  9.7499e-03, -4.0206e-03,\n",
       "         -4.3561e-03, -1.8670e-03,  1.8295e-03, -7.3207e-03, -2.9670e-03,\n",
       "          4.4422e-03, -4.9124e-03,  1.4957e-02, -5.0116e-03,  1.2069e-02,\n",
       "         -4.6408e-03, -1.2279e-02, -3.4053e-03,  9.6161e-03,  1.2978e-02,\n",
       "          6.0171e-03,  4.9745e-03,  4.1003e-03, -1.5143e-02,  4.8552e-03,\n",
       "         -1.2275e-03,  1.1815e-02,  1.4709e-02, -9.5444e-03, -1.7106e-02,\n",
       "         -2.2421e-03,  9.2703e-05,  1.2384e-02,  2.8150e-03,  7.4942e-03,\n",
       "         -4.0212e-03,  2.5488e-03, -3.4082e-03,  1.0657e-02,  1.4284e-02,\n",
       "          3.3492e-03, -4.9656e-03,  2.2917e-02, -8.4125e-03, -4.9140e-03,\n",
       "          2.8414e-04, -9.5653e-03,  1.0108e-02,  9.6165e-03,  1.4132e-02,\n",
       "          8.9793e-03,  4.5838e-03,  6.4050e-03,  1.2514e-02,  1.0897e-02,\n",
       "          8.9200e-04, -1.6242e-02, -6.9832e-03,  5.7330e-03, -5.2640e-03,\n",
       "         -1.2078e-02, -2.8292e-03, -1.5573e-02,  1.0236e-02, -1.2138e-02,\n",
       "         -8.7806e-03,  4.6415e-03, -1.6202e-02,  9.4935e-03, -1.2500e-02,\n",
       "          1.0828e-02,  1.2040e-02, -1.4525e-02, -3.4617e-03,  2.4764e-03,\n",
       "         -8.4179e-03,  1.0193e-02,  2.0969e-04,  1.2696e-02, -1.1247e-02,\n",
       "          1.4289e-02, -2.1987e-02, -1.5276e-02, -6.5545e-04, -4.8766e-03,\n",
       "          1.4681e-02,  1.9443e-02,  1.3785e-03, -1.7543e-02,  1.0836e-02,\n",
       "          3.9935e-03, -1.2192e-03, -8.2273e-03,  2.6057e-03,  3.5421e-03,\n",
       "          3.0827e-03, -1.3625e-02,  7.6229e-03, -2.0547e-03,  9.7590e-03,\n",
       "         -7.4800e-03,  7.3221e-03, -8.2276e-03, -7.5006e-03,  5.6557e-03,\n",
       "         -5.7717e-03,  1.1051e-02, -1.8859e-02,  3.8843e-03,  1.7465e-03,\n",
       "          1.0846e-02, -4.8358e-03,  8.1986e-03,  6.1561e-04,  1.1899e-02,\n",
       "          5.8042e-03,  9.8495e-03,  4.4352e-03, -8.5745e-04,  1.0517e-02,\n",
       "          4.2323e-03,  6.4556e-03,  6.1370e-03, -5.8809e-03, -8.9963e-03,\n",
       "          9.8873e-03, -1.7331e-03,  6.2954e-03, -2.0493e-03, -1.8892e-02,\n",
       "         -6.1904e-04, -1.2038e-02,  9.0967e-03,  1.1661e-02,  1.1833e-02,\n",
       "         -1.5613e-03,  4.1914e-03, -6.9798e-04, -1.1866e-03,  1.4729e-02,\n",
       "         -1.8445e-02,  1.3150e-02, -3.3549e-05, -6.1863e-04,  2.0107e-02,\n",
       "          1.5646e-02, -9.3266e-03, -3.3058e-03, -1.2447e-02,  6.8060e-04,\n",
       "         -7.3526e-03,  4.6711e-03, -1.8368e-02,  1.0627e-02, -1.4411e-02,\n",
       "         -1.6244e-02, -1.3413e-03, -5.0949e-03,  5.1821e-03, -7.1097e-03,\n",
       "         -1.4491e-03, -2.2282e-03,  8.7072e-03, -3.9756e-03,  1.2002e-02,\n",
       "          6.0356e-03, -1.1921e-02,  1.4451e-02, -1.6139e-02,  1.2675e-02,\n",
       "         -2.0593e-03,  7.9949e-03, -1.4570e-02,  6.5818e-03, -3.6176e-03,\n",
       "         -2.4814e-03,  6.9404e-03, -1.2520e-02,  8.6933e-03, -2.4541e-03,\n",
       "         -1.2806e-02, -1.2363e-02, -4.6156e-03, -4.6771e-03,  4.1753e-03,\n",
       "          1.0398e-02,  6.2658e-03,  8.4788e-03,  1.0934e-02, -1.2950e-02,\n",
       "         -1.2877e-03, -1.1608e-02,  1.8204e-02,  1.1401e-02, -8.7301e-03,\n",
       "         -3.4306e-04, -1.4970e-02, -3.9072e-03, -8.9765e-03, -9.7187e-03,\n",
       "          1.0534e-02, -9.1870e-03, -1.7243e-03, -1.1625e-02, -2.2214e-02,\n",
       "          5.6693e-05, -9.0557e-03,  1.9367e-03, -1.4082e-02,  1.2575e-03,\n",
       "         -1.5272e-02, -1.3085e-02, -5.1568e-03,  1.8291e-03,  3.4603e-03,\n",
       "         -9.6277e-03,  9.7794e-03,  7.4292e-03, -3.4994e-03,  4.2327e-03,\n",
       "         -9.2062e-03,  1.4514e-02, -9.0742e-03,  1.0966e-02,  1.0131e-02,\n",
       "         -6.0807e-03, -6.3429e-03, -6.0745e-03, -4.6537e-03,  1.4603e-02,\n",
       "         -1.2579e-02, -9.2546e-03, -2.9101e-03,  2.3867e-03, -1.8820e-03,\n",
       "          4.3344e-03, -6.9658e-03,  1.3779e-02,  4.9033e-03,  1.2816e-03,\n",
       "         -1.2608e-02, -6.0407e-03, -1.3326e-02,  1.5287e-02,  4.8038e-03,\n",
       "          1.1686e-02, -1.6169e-02,  1.2968e-02,  1.5812e-02,  1.5396e-02,\n",
       "          4.9861e-03, -1.4519e-02, -5.0628e-03, -2.5840e-03, -2.0093e-04,\n",
       "         -1.1553e-02,  8.2123e-03,  9.8385e-03,  2.4445e-03, -8.7901e-04,\n",
       "          3.6005e-03,  5.4704e-03,  8.3846e-03, -1.2154e-02,  1.4875e-02,\n",
       "          1.3986e-03,  7.9674e-03, -3.4484e-03, -1.6863e-02,  8.8054e-04,\n",
       "          2.8022e-03,  5.4282e-03, -6.1770e-03,  1.9509e-02,  4.8181e-03,\n",
       "         -1.4830e-02, -8.5544e-03, -5.3553e-03, -1.2954e-02,  1.0420e-02,\n",
       "          5.9861e-03,  1.1013e-02, -4.2574e-03, -1.2090e-02, -5.5118e-03,\n",
       "          1.7343e-02,  5.9283e-03, -2.0566e-03, -1.1634e-02,  1.6996e-03,\n",
       "         -3.9672e-03,  1.4848e-03, -1.5878e-02,  4.9841e-03, -5.5688e-04,\n",
       "          1.5759e-03, -4.1814e-03, -3.4926e-03,  8.8379e-03, -2.5046e-04,\n",
       "          8.7095e-03, -5.9487e-03,  1.1410e-02, -1.2649e-02,  1.2507e-02,\n",
       "          1.0431e-02, -1.4715e-02, -3.1933e-03,  9.8057e-03,  1.4442e-02,\n",
       "         -5.7576e-03,  3.1697e-03, -1.2479e-02, -5.1340e-04, -4.5014e-03,\n",
       "          1.1510e-02,  4.0895e-03, -1.2212e-03,  8.6717e-03, -6.2240e-03,\n",
       "         -1.2721e-03,  5.4634e-03, -1.6520e-02, -5.9253e-03,  1.8227e-02,\n",
       "          5.3209e-03,  1.1811e-02, -1.3276e-02,  4.0461e-03, -5.5852e-03,\n",
       "          2.1769e-02, -6.3753e-03,  2.1166e-03, -6.9000e-03, -1.3469e-02,\n",
       "          1.2281e-02, -1.7234e-02, -6.9643e-03, -9.4377e-03, -1.1061e-02,\n",
       "         -9.9441e-04, -3.1686e-03, -7.2093e-03, -8.6628e-03, -9.4313e-03,\n",
       "         -1.3511e-02, -1.6548e-03, -3.7335e-05,  1.0587e-02,  1.0933e-02,\n",
       "          6.3507e-04, -1.9901e-02, -4.8830e-03, -3.1582e-03, -1.2026e-03,\n",
       "         -5.5481e-03,  9.3665e-03,  9.9270e-03,  9.0464e-03,  1.2534e-02,\n",
       "         -7.7860e-03,  9.7895e-03, -2.3148e-03, -1.4095e-02,  3.2823e-03,\n",
       "         -4.2469e-03,  2.1778e-03,  7.7691e-03,  8.3602e-03, -7.8500e-03,\n",
       "          1.4760e-02,  4.2905e-03,  9.8937e-03,  3.6250e-03,  3.2724e-04,\n",
       "          4.6178e-03, -9.4232e-03,  1.4136e-02,  8.2277e-03,  9.5398e-03,\n",
       "         -2.1060e-02, -1.4389e-02, -1.1328e-02,  7.1373e-03, -4.9734e-03,\n",
       "          6.6172e-04,  1.0002e-02,  1.2896e-02, -4.0276e-03, -6.5320e-03,\n",
       "          1.1133e-02, -1.1336e-03, -1.0994e-02, -1.2137e-02, -7.0901e-03,\n",
       "          1.7795e-02,  1.2514e-02,  1.3628e-02,  4.2759e-03,  1.3820e-02,\n",
       "          9.5918e-03, -1.3547e-02, -3.5412e-03, -8.5843e-03,  3.1734e-03,\n",
       "          1.1414e-02, -2.9532e-03, -1.1487e-02,  7.7596e-03, -1.8648e-02,\n",
       "         -1.7354e-02,  8.2607e-03, -8.7456e-03,  2.5041e-03, -7.3882e-03,\n",
       "          1.0927e-02,  2.0482e-02, -7.6514e-04, -1.6257e-02,  6.4428e-03,\n",
       "          1.6617e-02,  1.2016e-02,  9.8173e-04, -5.2572e-03,  2.3602e-03,\n",
       "         -9.0330e-03, -1.3843e-02,  5.5001e-03, -1.0209e-02, -2.4358e-03,\n",
       "         -8.8524e-03, -1.6979e-04,  1.4435e-02, -1.6577e-02,  1.7793e-02,\n",
       "          5.7865e-03, -1.2581e-02, -1.1019e-02, -1.8335e-04,  9.4592e-03,\n",
       "         -1.5563e-02, -9.2428e-03, -2.4533e-03, -4.9645e-04,  1.2937e-02,\n",
       "          7.1866e-03,  1.5671e-02,  1.0458e-02, -1.9396e-02, -1.4659e-02,\n",
       "         -1.2369e-02,  4.7377e-03, -1.3317e-02,  1.4962e-02, -7.7392e-04,\n",
       "         -1.6732e-03, -3.5725e-03, -4.6595e-03, -5.1598e-03,  1.3917e-02,\n",
       "         -1.3966e-02,  1.7642e-02, -7.7038e-03,  1.0994e-03,  9.7044e-03,\n",
       "          1.3189e-02, -1.0399e-02,  7.8789e-03, -1.9736e-03, -1.0263e-02,\n",
       "         -1.2673e-02,  1.3655e-02, -4.3886e-05,  7.6022e-03,  1.3008e-02,\n",
       "          2.1478e-02,  1.2651e-02, -8.0174e-03,  7.3870e-03,  2.3270e-04,\n",
       "         -4.9458e-03, -9.3174e-03,  3.3104e-03, -4.7059e-03, -3.0289e-03,\n",
       "          9.0131e-03, -1.0946e-02, -1.0611e-02,  1.0429e-03,  7.3754e-03,\n",
       "         -3.7676e-03, -1.9673e-02, -4.1407e-05,  6.5972e-03, -8.0501e-03,\n",
       "         -1.6415e-03, -4.8643e-03,  7.6928e-03,  1.1028e-02, -7.0078e-03,\n",
       "          8.2398e-03, -4.4702e-03,  3.7006e-03,  6.7963e-03, -1.2729e-02,\n",
       "          1.1361e-02,  1.2624e-02,  1.2290e-02, -1.0024e-02, -2.0961e-04,\n",
       "          6.3957e-03, -8.3590e-03, -1.3781e-02,  5.8763e-03,  1.8508e-02,\n",
       "          1.5644e-03,  4.2840e-03,  9.1838e-03,  1.3105e-02, -4.2629e-03,\n",
       "          1.7179e-02, -1.0703e-02,  7.7326e-03,  1.6629e-02, -1.5304e-03,\n",
       "         -4.9369e-03,  1.5578e-02, -1.6517e-02,  5.7003e-03,  2.8312e-03,\n",
       "          1.0833e-02,  1.2833e-02,  2.3303e-03,  7.6012e-03,  9.4006e-03,\n",
       "         -3.6722e-03, -1.4240e-02, -2.7141e-03,  1.0165e-02, -1.2874e-02,\n",
       "          1.2128e-02,  1.0247e-02, -1.1471e-02,  1.1556e-02,  1.5421e-02,\n",
       "          1.1410e-02,  1.1535e-02, -6.0740e-03,  3.2640e-03,  2.1583e-02,\n",
       "         -1.9914e-02,  1.0438e-02, -1.1733e-02, -1.3873e-02,  1.4687e-02,\n",
       "          6.3572e-03,  1.7177e-04,  1.3770e-03,  1.4164e-02,  3.2800e-03,\n",
       "         -7.0358e-03, -2.0488e-03, -3.0089e-04,  8.7452e-03,  1.3557e-02,\n",
       "         -5.6229e-03,  9.9140e-03,  9.9808e-03, -1.7727e-02, -1.8537e-02,\n",
       "          1.9472e-02, -9.8436e-03,  2.1990e-03,  1.5598e-02,  6.9358e-03,\n",
       "          1.4882e-02, -8.7182e-03, -9.3433e-03, -8.0185e-03,  1.1705e-03,\n",
       "         -7.1481e-03, -8.9714e-03,  1.3893e-02,  8.6557e-03, -1.1880e-02,\n",
       "         -4.9689e-03, -1.8454e-03,  1.8076e-02,  1.2228e-02, -7.7241e-03,\n",
       "          4.6519e-03, -8.5650e-03, -8.5823e-03,  1.5561e-02,  7.7892e-03,\n",
       "         -7.8440e-03, -9.9142e-03,  1.6245e-02,  2.0642e-02, -4.7733e-03,\n",
       "         -5.9714e-03, -1.4572e-02,  4.4296e-03, -7.2234e-03, -2.9128e-03,\n",
       "         -9.0342e-03, -5.8788e-03, -5.2460e-03, -1.4595e-02, -1.3297e-02,\n",
       "          1.6958e-02, -1.3150e-02,  1.9805e-02,  3.3214e-03, -3.8476e-03,\n",
       "          3.8450e-03,  6.9339e-03, -3.7879e-03, -5.2739e-03,  4.1833e-03,\n",
       "          1.1174e-02, -8.2210e-03,  1.0094e-02, -1.2170e-02,  5.3359e-03,\n",
       "          1.2379e-02, -3.0868e-03, -8.2497e-03,  5.9158e-03, -1.0427e-02,\n",
       "          1.2286e-02, -1.5175e-02,  1.7333e-02, -2.2887e-02, -3.8797e-03,\n",
       "         -2.0536e-02, -1.9053e-02,  7.3080e-03, -1.3102e-03,  1.6295e-03,\n",
       "          3.6164e-03,  1.3205e-03,  4.0316e-03,  1.2092e-02, -1.3430e-02,\n",
       "         -1.7484e-02,  6.9260e-03, -7.1026e-03,  7.5427e-03,  6.2308e-03,\n",
       "          3.4998e-03,  5.7306e-03,  1.1728e-02,  4.3391e-03, -1.3700e-02,\n",
       "          1.5223e-03,  2.8855e-03,  5.1034e-03,  1.8225e-02,  8.4651e-03,\n",
       "         -5.9364e-04, -7.4391e-03, -8.4273e-03,  3.8508e-03, -1.8403e-03,\n",
       "          8.1080e-03, -1.5360e-02, -1.1070e-02, -9.1457e-03,  5.9682e-03,\n",
       "         -5.1636e-03, -6.0431e-03, -6.2096e-03, -5.5092e-03,  5.6533e-03,\n",
       "          9.8665e-03, -1.3407e-02, -9.3002e-03,  5.9676e-03, -8.6973e-03,\n",
       "         -6.8501e-03,  5.5307e-03, -1.9544e-03, -8.1342e-03, -2.0940e-02,\n",
       "          3.4940e-03,  1.7023e-02, -7.5940e-03,  1.1873e-02, -1.8618e-02,\n",
       "          1.3218e-02,  2.8580e-04,  4.8863e-03,  9.0595e-03,  2.3650e-03,\n",
       "          4.6166e-03, -1.4076e-02,  4.9325e-03,  1.0419e-02,  8.9055e-03,\n",
       "         -8.2526e-03, -1.0461e-02, -1.7173e-02,  8.0254e-03, -1.3903e-02,\n",
       "         -1.1636e-02, -4.1327e-03,  7.6720e-03, -8.7019e-04, -3.2040e-03,\n",
       "         -1.1869e-02, -1.6184e-02, -1.3565e-02, -7.9791e-03,  6.9414e-03,\n",
       "         -4.9665e-03, -1.5584e-02,  3.2984e-03,  9.8535e-03,  6.3146e-03,\n",
       "         -4.5395e-03, -6.1161e-03,  1.7674e-02, -1.9994e-02,  2.7702e-03,\n",
       "         -1.3997e-02,  4.6219e-03, -9.7892e-03,  6.1008e-03,  3.8267e-03,\n",
       "         -6.5016e-03, -9.4497e-03, -9.3461e-03, -2.0603e-02, -1.2228e-03,\n",
       "          5.1648e-03, -1.0556e-02, -1.2672e-02,  6.7362e-03, -3.7655e-03,\n",
       "          4.8595e-03,  1.1037e-02, -2.4051e-03,  1.4750e-03, -1.0232e-03,\n",
       "         -9.4139e-03,  8.6785e-03,  5.4163e-03, -2.8920e-03,  1.7456e-02,\n",
       "          4.9356e-03,  1.0226e-02,  3.8750e-03,  1.3986e-02, -7.5060e-03,\n",
       "          8.8798e-03, -7.6517e-03,  6.0595e-03,  8.9086e-03, -4.6955e-03,\n",
       "         -1.2832e-02,  4.6999e-03,  1.4747e-02, -6.3018e-03,  5.0307e-03,\n",
       "          3.0688e-03,  2.7994e-03, -7.3050e-03, -7.0427e-03, -4.8809e-03,\n",
       "         -1.0565e-02,  7.3432e-04, -1.0797e-02, -1.0178e-02,  1.1314e-02,\n",
       "          1.2339e-02,  9.3839e-04,  1.1134e-02,  6.7254e-03, -7.6265e-03,\n",
       "         -1.3993e-02, -5.0685e-03,  7.4679e-04,  5.4350e-03,  1.7192e-02,\n",
       "         -1.0771e-02, -9.5697e-03, -4.4900e-04,  1.1054e-02,  1.4934e-02,\n",
       "          1.8166e-02,  4.4355e-03,  1.1457e-02, -3.2065e-03, -8.4732e-04,\n",
       "          3.9072e-03,  1.0107e-02, -9.6163e-03,  5.1123e-03,  2.1945e-02,\n",
       "         -4.1609e-04, -1.0594e-03,  1.0480e-03,  1.9637e-02, -1.9025e-02,\n",
       "         -5.2179e-03, -1.2538e-02,  1.1651e-02,  1.6206e-02, -1.1819e-02,\n",
       "          1.2470e-02, -6.9961e-03,  1.2044e-02,  8.7471e-03,  1.6220e-02,\n",
       "          1.3603e-02, -3.1707e-03,  5.0070e-03,  5.3054e-03,  1.1215e-02,\n",
       "          1.0707e-02, -1.3987e-02,  1.1463e-02, -9.6940e-04, -1.3304e-02,\n",
       "          1.3325e-02,  1.0559e-02,  1.0328e-02,  7.9229e-04, -3.1276e-03,\n",
       "          8.0244e-03,  6.5241e-03,  1.8810e-03,  5.0231e-03, -4.0783e-03,\n",
       "         -6.6827e-03, -4.4132e-04, -9.5263e-03,  1.1439e-02,  7.0771e-03,\n",
       "         -1.2679e-02,  1.1185e-02, -1.5553e-02,  1.0954e-02, -1.7829e-02,\n",
       "          1.0760e-02, -7.7756e-03,  7.5647e-04,  1.3190e-02, -9.5153e-03,\n",
       "          8.7598e-03,  9.0753e-03, -1.4677e-02,  4.1945e-03, -7.4099e-03,\n",
       "         -2.8881e-03, -3.8596e-03, -1.6492e-02, -8.9492e-03,  9.2590e-03,\n",
       "          2.8243e-03,  3.3641e-03, -2.7732e-03, -1.1224e-02, -1.0181e-02,\n",
       "          1.9068e-02,  1.3841e-02, -1.5519e-02, -7.8638e-03,  1.3596e-02,\n",
       "         -8.4289e-03,  1.1192e-02, -3.4567e-03, -4.5408e-03, -9.0035e-03,\n",
       "         -6.7617e-04, -6.8304e-03, -4.2534e-03, -1.9315e-02, -1.4869e-02,\n",
       "         -6.9566e-03,  3.7534e-03, -9.8533e-03,  1.2261e-02,  1.0722e-03,\n",
       "          1.5790e-03,  3.2725e-03, -1.2011e-03, -1.1390e-02, -2.0201e-02,\n",
       "         -1.4872e-02, -2.8485e-04, -1.0473e-02,  1.7141e-02,  1.4463e-02,\n",
       "         -1.1582e-02, -1.0378e-02, -7.7796e-03, -5.4621e-03,  1.2080e-02,\n",
       "          3.0572e-03,  7.0623e-03, -1.2071e-02, -5.5518e-03, -1.1295e-02,\n",
       "          2.9281e-03,  3.1415e-03, -2.9822e-03,  5.6354e-03, -5.7496e-03,\n",
       "          9.5469e-03, -1.8366e-02, -5.3122e-03,  4.7743e-04,  3.6307e-04,\n",
       "         -6.2473e-03, -1.3976e-02,  4.9059e-03,  1.5084e-02,  6.3805e-03,\n",
       "          7.3569e-03, -3.3560e-03, -1.2078e-02, -9.1099e-03, -1.5658e-03,\n",
       "         -4.7049e-03, -9.8703e-03, -2.3301e-03, -1.4235e-02,  2.7844e-03,\n",
       "         -1.7556e-03,  8.3617e-03,  1.3447e-02, -4.0993e-03, -1.2081e-02,\n",
       "         -5.5287e-03, -3.0217e-03,  2.5134e-03,  7.8886e-03, -9.5905e-04,\n",
       "          1.0849e-02,  1.1843e-02,  1.0123e-02, -4.3953e-03, -6.4490e-03,\n",
       "         -1.0537e-02,  5.8319e-03, -3.7569e-03, -1.0766e-03, -7.9484e-03,\n",
       "         -1.3355e-02,  8.1731e-03,  4.2813e-04, -6.9614e-03, -1.2302e-03,\n",
       "         -3.0730e-03,  8.8620e-03,  1.6737e-02, -1.8792e-02, -1.6058e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image:\", input_image.shape)\n",
    "\n",
    "vgg = VGG16_v2()\n",
    "vgg.__init__()\n",
    "vgg.forward(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer 하나씩 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 하나씩 만들기\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(9*9*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "            \n",
    "    def forward(self, x):\n",
    "#         out = F.relu(self.conv2(x)), (2, 2)\n",
    "#         out = F.relu(self.conv4(x)), (2, 2)\n",
    "#         out = F.relu(self.conv6(x)), (2, 2)\n",
    "#         out = F.relu(self.conv9(x)), (2, 2)\n",
    "#         out = F.relu(self.conv12(x)), (2, 2)\n",
    "\n",
    "        x = F.relu(self.pool1(x)), (2, 2)\n",
    "        x = F.relu(self.pool2(x)), (2, 2)\n",
    "        x = F.relu(self.pool3(x)), (2, 2)\n",
    "        x = F.relu(self.pool4(x)), (2, 2)\n",
    "        x = F.relu(self.pool5(x)), (2, 2)\n",
    "        \n",
    "        x=x.view(out.size(0), -1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        print(x.shape)\n",
    "        return x\n",
    "        \n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image:  torch.Size([1, 3, 300, 300])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max_pool2d(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c694c03f0e1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvgg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# out1 = layer1(in_image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(out1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f228376ad369>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    158\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 576\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mTypeError\u001b[0m: max_pool2d(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "in_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image: \", in_image.shape)\n",
    "\n",
    "\n",
    "vgg = VGG16()\n",
    "vgg.__init__()\n",
    "vgg.forward(in_image)\n",
    "# out1 = layer1(in_image)\n",
    "# print(out1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잘 모르겠어서 한줄씩 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image:  torch.Size([1, 3, 300, 300])\n",
      "conv1: torch.Size([1, 64, 300, 300])\n",
      "conv2: torch.Size([1, 64, 300, 300])\n",
      "pool1:  torch.Size([1, 64, 150, 150])\n",
      "\n",
      "conv3: torch.Size([1, 128, 150, 150])\n",
      "conv4: torch.Size([1, 128, 150, 150])\n",
      "pool2:  torch.Size([1, 128, 75, 75])\n",
      "\n",
      "conv5: torch.Size([1, 256, 75, 75])\n",
      "conv6: torch.Size([1, 256, 75, 75])\n",
      "pool3:  torch.Size([1, 256, 37, 37])\n",
      "\n",
      "conv7: torch.Size([1, 512, 37, 37])\n",
      "conv8: torch.Size([1, 512, 37, 37])\n",
      "pool4:  torch.Size([1, 512, 18, 18])\n",
      "\n",
      "conv9: torch.Size([1, 512, 18, 18])\n",
      "conv10: torch.Size([1, 512, 18, 18])\n",
      "pool5:  torch.Size([1, 512, 9, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image: \", in_image.shape)\n",
    "\n",
    "conv1 = nn.Conv2d(3, 64, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "feature_map1 = conv1(in_image)\n",
    "print(\"conv1:\", feature_map1.shape)\n",
    "feature_map1 = conv2(feature_map1)\n",
    "print(\"conv2:\", feature_map1.shape)\n",
    "feature_map2 = pool(feature_map1)\n",
    "print(\"pool1: \", feature_map2.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "conv3 = nn.Conv2d(64, 128, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "feature_map3 = conv3(feature_map2)\n",
    "print(\"conv3:\", feature_map3.shape)\n",
    "feature_map3 = conv4(feature_map3)\n",
    "print(\"conv4:\", feature_map3.shape)\n",
    "feature_map4 = pool(feature_map3)\n",
    "print(\"pool2: \", feature_map4.shape)\n",
    "print(\"\")\n",
    "\n",
    "conv5 = nn.Conv2d(128, 256, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "feature_map5 = conv5(feature_map4)\n",
    "print(\"conv5:\", feature_map5.shape)\n",
    "feature_map5 = conv6(feature_map5)\n",
    "print(\"conv6:\", feature_map5.shape)\n",
    "feature_map6 = pool(feature_map5)\n",
    "print(\"pool3: \", feature_map6.shape)\n",
    "print(\"\")\n",
    "\n",
    "conv7 = nn.Conv2d(256, 512, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "feature_map7 = conv7(feature_map6)\n",
    "print(\"conv7:\", feature_map7.shape)\n",
    "feature_map7 = conv8(feature_map7)\n",
    "print(\"conv8:\", feature_map7.shape)\n",
    "feature_map8 = pool(feature_map7)\n",
    "print(\"pool4: \", feature_map8.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "conv9 = nn.Conv2d(512, 512, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv10 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "feature_map9 = conv9(feature_map8)\n",
    "print(\"conv9:\", feature_map9.shape)\n",
    "feature_map9 = conv10(feature_map9)\n",
    "print(\"conv10:\", feature_map9.shape)\n",
    "feature_map10 = pool(feature_map9)\n",
    "print(\"pool5: \", feature_map10.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ny_torch",
   "language": "python",
   "name": "ny_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
