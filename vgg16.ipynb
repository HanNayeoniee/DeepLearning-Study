{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import scipy\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "batch_size=16\n",
    "learning_rate=0.002\n",
    "num_epoch=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential로 묶어서 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. sequential 1개에 묶어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번에 다 묶기\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "       \n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "          \n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(9*9*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.layer(x)\n",
    "        print(out.shape)\n",
    "#         out=out.view(batch_size, -1)\n",
    "        out=out.view(out.size(0), -1)\n",
    "        out=self.fc_layer(out)\n",
    "        print(\"최종 shape:\", out.shape)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image: torch.Size([1, 3, 300, 300])\n",
      "torch.Size([1, 512, 9, 9])\n",
      "최종 shape: torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9135e-02, -1.2884e-02, -7.7258e-03, -9.7490e-03, -5.2360e-03,\n",
       "          5.1209e-03, -1.5796e-02, -1.7118e-02, -2.7389e-03,  1.4944e-02,\n",
       "         -2.3244e-05,  9.5100e-04, -7.5765e-03, -1.2704e-02, -4.3363e-03,\n",
       "         -3.4848e-03,  1.0254e-02,  4.9543e-03, -5.9576e-05,  1.5761e-02,\n",
       "         -5.0037e-03, -2.0418e-02, -1.8029e-03, -1.1825e-02, -6.3098e-03,\n",
       "         -6.4095e-03,  2.6481e-03,  1.8443e-04, -6.4565e-03, -2.5426e-03,\n",
       "         -1.3781e-02,  3.3567e-03, -6.6789e-03,  1.0852e-02, -8.8739e-03,\n",
       "         -1.5266e-03, -1.4970e-02,  5.0213e-03, -1.9098e-02, -1.1604e-02,\n",
       "          1.4490e-02,  9.3814e-04, -1.5885e-02, -5.3039e-03,  9.3469e-03,\n",
       "         -7.0953e-03,  8.8654e-03,  5.0800e-03, -3.1279e-03, -1.0293e-02,\n",
       "          1.4120e-02,  1.7324e-02, -1.0021e-02,  1.7133e-02, -1.8690e-02,\n",
       "         -1.1692e-03,  1.1450e-02, -1.6734e-02,  1.4426e-02, -8.4639e-04,\n",
       "          1.9435e-02,  2.0823e-04,  1.4763e-02,  3.5228e-03, -1.8883e-03,\n",
       "         -3.5747e-03,  5.9296e-03, -8.6725e-03,  1.3521e-02,  9.4044e-03,\n",
       "         -1.5989e-02,  8.1403e-03,  6.3473e-03, -6.9872e-03,  5.7842e-03,\n",
       "          5.4999e-03,  1.8684e-02,  6.9389e-03, -6.0096e-03, -8.6119e-05,\n",
       "         -8.9352e-03,  1.2008e-02,  3.3963e-03,  1.6969e-02,  8.0793e-03,\n",
       "         -1.0640e-02, -3.1971e-03,  1.8506e-03, -2.9772e-03,  7.7678e-03,\n",
       "         -1.7906e-02, -2.5086e-03, -8.0437e-03,  5.9040e-03, -2.1559e-02,\n",
       "         -1.2920e-02, -1.1311e-03,  1.1790e-02,  1.5041e-02,  4.7207e-03,\n",
       "         -1.5019e-02,  2.2400e-03, -9.4295e-04,  6.6515e-03, -1.1006e-02,\n",
       "          8.0805e-03,  1.3657e-02, -1.2455e-02,  1.2611e-02, -1.5216e-02,\n",
       "          1.7415e-02, -1.4643e-02, -8.4512e-03, -1.0070e-02,  8.3667e-03,\n",
       "          1.1332e-02,  1.2169e-02,  7.6066e-03,  9.3879e-03, -4.8459e-03,\n",
       "         -2.1377e-02,  8.8342e-03,  8.5946e-03,  3.2786e-03,  5.2216e-03,\n",
       "          1.4425e-02,  1.2324e-02,  4.2694e-03, -2.2367e-02,  5.8780e-03,\n",
       "          7.0832e-03,  6.1526e-03, -9.2037e-03, -1.1432e-02,  9.5230e-03,\n",
       "          8.5892e-03, -1.6003e-03,  4.9826e-03, -1.2066e-02, -1.6586e-02,\n",
       "          1.3174e-02,  1.5864e-02, -8.2300e-03,  8.0417e-03, -1.9252e-02,\n",
       "          3.6127e-03, -1.0261e-02,  4.4224e-03, -1.2066e-02, -7.0073e-03,\n",
       "         -5.9637e-03, -7.3260e-03,  9.1990e-04,  1.4911e-02, -7.4647e-03,\n",
       "          1.5590e-02,  8.1632e-03,  9.6503e-03, -4.5227e-03,  6.9979e-03,\n",
       "         -2.3366e-02,  6.0216e-03,  3.2504e-03, -1.0989e-02,  5.5180e-03,\n",
       "         -1.2590e-02, -6.2440e-03, -1.6465e-02, -1.1455e-02, -1.4438e-02,\n",
       "         -5.0021e-04,  2.1583e-03,  8.6117e-03, -1.2885e-02,  1.1662e-02,\n",
       "          1.0534e-02, -1.8552e-03,  2.0329e-03,  3.8744e-03, -1.0474e-02,\n",
       "          7.2699e-03,  8.7170e-03, -1.5387e-03, -4.3458e-03, -3.3464e-03,\n",
       "          7.9191e-03, -7.3937e-03, -1.9560e-02, -1.1850e-02, -2.8555e-03,\n",
       "         -3.0976e-03, -5.1049e-03, -9.7803e-03, -3.2511e-04,  6.4079e-03,\n",
       "         -4.3284e-03,  6.6624e-03,  1.0935e-02,  1.4632e-02,  5.0761e-03,\n",
       "          5.8614e-03, -4.3851e-03, -1.0260e-04,  3.2210e-03, -4.5449e-03,\n",
       "         -1.2399e-02, -6.6688e-03, -4.3432e-03,  6.8339e-03,  7.3598e-03,\n",
       "         -2.9655e-03, -2.1998e-03,  1.7303e-02, -1.0025e-03,  5.1656e-03,\n",
       "          1.0855e-02,  1.1871e-02,  6.0084e-03, -1.5382e-02,  6.5427e-03,\n",
       "         -8.5279e-03,  1.1009e-03,  1.6021e-02,  2.4828e-03,  1.0007e-02,\n",
       "          4.2006e-03,  6.2384e-03,  5.9170e-04,  4.2926e-03,  9.0165e-03,\n",
       "         -1.6101e-02,  1.4092e-03, -1.2844e-03, -2.7326e-03, -8.8470e-03,\n",
       "          1.6011e-02,  1.4006e-02,  1.2470e-02, -1.0353e-02,  1.0621e-02,\n",
       "          1.4266e-02, -1.3914e-02,  5.5035e-03, -1.0724e-02,  2.3812e-03,\n",
       "         -6.6580e-04,  1.4137e-02, -8.9712e-03,  1.5327e-02, -1.0317e-02,\n",
       "         -9.2811e-03,  3.0204e-03,  7.8598e-04,  9.1809e-04,  3.9534e-03,\n",
       "         -8.9407e-03, -1.4772e-02, -1.3878e-02,  5.0072e-03,  1.0437e-02,\n",
       "         -7.0771e-03, -1.2021e-02, -1.8223e-02,  1.0951e-02,  1.3266e-02,\n",
       "         -6.1253e-03,  7.1164e-03, -1.3765e-02, -2.5772e-04, -9.2017e-03,\n",
       "         -5.8154e-03,  8.4483e-03, -1.5426e-02,  4.1655e-03, -5.9101e-03,\n",
       "         -4.8722e-03, -5.1358e-03, -5.0078e-03, -3.4574e-03, -1.3221e-03,\n",
       "          1.4090e-02, -1.7361e-02,  9.5655e-03,  1.9023e-02,  2.4296e-02,\n",
       "         -1.0800e-02, -1.4468e-02,  8.9354e-03,  1.5757e-04, -2.2860e-02,\n",
       "         -8.8438e-03,  5.1742e-03, -1.5079e-03, -9.3633e-03,  6.8630e-03,\n",
       "         -1.1738e-02,  1.4338e-02,  1.2717e-02, -3.0486e-03,  2.0117e-02,\n",
       "          2.0944e-02, -7.0661e-03,  1.0988e-02,  3.7290e-03, -5.3698e-03,\n",
       "         -6.8207e-03,  4.7623e-03, -1.5171e-02,  1.5718e-02, -8.4717e-03,\n",
       "          7.0023e-03,  1.1027e-02,  1.3272e-02, -8.7493e-03, -8.2154e-03,\n",
       "          5.9149e-03,  6.5747e-03,  4.8185e-03, -1.0998e-02, -1.4535e-02,\n",
       "          1.2078e-02,  1.2766e-02,  3.3504e-03,  9.2474e-04, -1.5457e-02,\n",
       "         -1.2795e-02,  1.1724e-02, -8.8604e-03,  1.0853e-02,  6.8917e-03,\n",
       "         -1.7528e-03,  1.0734e-02, -7.6113e-03,  1.0510e-02,  4.1202e-03,\n",
       "          2.0704e-03,  5.2767e-03,  7.7577e-03, -3.0076e-03, -1.8377e-03,\n",
       "         -3.5637e-03, -1.5904e-03, -1.4378e-02, -2.0060e-03, -1.2418e-02,\n",
       "          8.2129e-03,  2.5714e-03,  3.6340e-03,  1.4452e-02,  1.1583e-02,\n",
       "          9.8188e-03, -1.8013e-02,  4.0921e-03, -2.1665e-03, -2.8246e-03,\n",
       "          9.1228e-04,  1.2448e-02, -6.0711e-03, -1.3606e-02,  1.6455e-02,\n",
       "         -2.8475e-03, -1.4038e-02,  3.6673e-03, -4.7572e-04,  1.2663e-02,\n",
       "          7.3174e-03,  1.0727e-03,  5.8725e-03,  1.2525e-02,  5.8492e-04,\n",
       "          1.3395e-02,  1.4141e-02, -1.0730e-03, -1.5254e-02, -1.2670e-02,\n",
       "          9.6259e-03, -1.2819e-02,  7.3043e-03,  7.3778e-03,  1.4651e-02,\n",
       "         -5.2746e-04, -1.1715e-02,  8.5346e-03,  4.4629e-03, -9.1962e-03,\n",
       "          5.3481e-03, -5.9179e-03,  9.4201e-03,  7.9571e-03,  2.4174e-03,\n",
       "         -1.1903e-02,  1.6028e-02,  1.8565e-04, -1.6104e-02, -2.5508e-02,\n",
       "          2.7668e-03,  1.0386e-02, -8.9844e-03,  7.2849e-04, -4.5191e-03,\n",
       "         -1.5470e-02, -7.3984e-03,  1.3880e-02,  1.3718e-02, -9.6260e-03,\n",
       "          2.2287e-02, -4.5469e-03,  9.0608e-03, -4.0015e-03,  7.1050e-03,\n",
       "          2.0172e-02, -6.3826e-03, -1.2078e-02, -5.4310e-03,  7.0033e-03,\n",
       "          1.1013e-02, -2.0009e-03, -1.3323e-02,  7.3338e-03,  4.7597e-03,\n",
       "         -7.0272e-03, -5.3396e-04,  7.4365e-03, -4.6359e-03, -1.3411e-02,\n",
       "         -6.2780e-03,  3.1850e-03, -6.5108e-04, -7.0221e-04,  4.6783e-03,\n",
       "          1.4540e-02,  1.5930e-02, -5.1898e-03,  5.4115e-03, -6.4600e-03,\n",
       "         -1.0743e-02,  2.7197e-03,  5.7912e-03,  1.9650e-02, -6.5639e-03,\n",
       "         -6.5430e-03,  8.0446e-03, -1.5392e-02,  3.3752e-03,  1.2895e-02,\n",
       "         -7.7211e-03,  1.0846e-02, -1.2810e-02, -7.5777e-03,  1.5309e-02,\n",
       "          5.3085e-03, -2.1910e-03,  3.9659e-03, -1.5614e-02, -3.8094e-03,\n",
       "          1.9604e-02,  1.5115e-02,  2.0178e-03,  4.5943e-03,  1.2191e-02,\n",
       "         -2.8191e-03,  5.1352e-03,  3.0226e-03,  2.3215e-03,  1.5938e-02,\n",
       "         -1.7646e-02,  8.5357e-03, -1.8116e-03, -1.6359e-02, -8.9414e-03,\n",
       "         -8.9064e-03,  9.5419e-03,  1.4936e-03,  1.0829e-02,  9.4760e-03,\n",
       "         -5.2379e-03,  9.0185e-03, -8.9325e-04, -5.3231e-03,  1.3801e-02,\n",
       "          1.3321e-02,  1.4654e-02, -8.6513e-03, -1.3828e-02,  2.8026e-03,\n",
       "         -1.1682e-02,  6.6225e-03,  5.3690e-03, -3.2033e-03, -9.1588e-03,\n",
       "         -1.2145e-03, -2.5983e-03,  1.5874e-03, -4.0468e-04,  4.8901e-03,\n",
       "         -1.0630e-02,  7.0429e-03, -2.9273e-03, -1.3503e-02,  1.1087e-02,\n",
       "          1.8054e-03,  6.1219e-03,  5.4744e-03, -9.7436e-03, -1.3998e-02,\n",
       "          8.6578e-03, -9.1787e-03, -4.2919e-03,  2.1160e-02, -8.6901e-03,\n",
       "         -2.1675e-03, -1.7753e-02, -8.8487e-03,  2.8567e-03,  1.5886e-02,\n",
       "          8.4653e-03,  1.3706e-02,  4.4672e-04, -1.9760e-02, -7.0668e-03,\n",
       "         -2.0551e-04,  1.1054e-02, -1.0829e-02,  1.6321e-02,  2.7449e-03,\n",
       "         -2.5693e-03, -1.1677e-02, -6.2427e-03,  6.6621e-03, -8.6557e-03,\n",
       "          1.1984e-02, -1.0708e-02,  1.8097e-02,  6.2524e-03,  2.0256e-02,\n",
       "         -9.9829e-03, -1.9290e-02, -8.3871e-03,  7.8718e-03,  9.9997e-03,\n",
       "          1.2795e-03,  2.7447e-03, -6.5943e-03,  1.3118e-02,  1.2736e-03,\n",
       "         -1.0602e-02, -1.5276e-02, -5.8468e-03, -8.0957e-03,  3.9754e-03,\n",
       "         -1.1484e-02, -1.6166e-02,  9.8447e-03,  2.5315e-03, -3.6891e-03,\n",
       "          8.0151e-03, -8.2629e-03,  1.4706e-02, -6.5005e-03, -1.1454e-02,\n",
       "         -1.2677e-02, -1.4632e-02, -1.6847e-02,  1.6131e-02, -9.3322e-03,\n",
       "         -9.6956e-03,  1.3621e-02, -1.8605e-03, -1.2092e-02, -1.8488e-03,\n",
       "         -1.0023e-02, -8.5597e-03,  1.8695e-02,  1.2222e-02,  1.0427e-02,\n",
       "         -1.6411e-02, -1.3386e-02, -5.1017e-03, -1.1337e-02, -1.5873e-02,\n",
       "         -8.5516e-03, -7.6351e-03,  4.2100e-03,  2.3518e-03, -2.6472e-03,\n",
       "          1.0013e-02,  6.7203e-03, -5.0926e-03, -1.1987e-03,  2.5262e-05,\n",
       "         -1.5098e-02,  1.4781e-02, -1.2579e-02,  6.2043e-03, -1.4357e-02,\n",
       "          1.6292e-02, -8.8558e-03,  4.2657e-03, -1.2772e-02, -6.6020e-03,\n",
       "         -2.9169e-03,  2.3720e-03, -8.8676e-03, -1.4309e-02, -1.2221e-02,\n",
       "          5.1115e-03, -1.2483e-02, -8.2440e-04, -8.8683e-03,  1.5627e-02,\n",
       "          4.2250e-03, -3.7278e-03, -1.9875e-03,  9.0033e-03,  1.0028e-02,\n",
       "         -5.4210e-03,  1.4009e-02, -6.2873e-03,  2.3626e-04,  1.0561e-02,\n",
       "          1.6785e-03, -1.2262e-02, -1.5420e-03, -3.6871e-04,  9.1944e-03,\n",
       "         -1.2073e-02, -1.5746e-02,  9.4364e-03,  3.8332e-03, -9.5607e-03,\n",
       "         -1.0865e-02, -1.1251e-02,  8.6997e-03, -1.0261e-02, -1.0087e-02,\n",
       "          1.5162e-02, -1.2718e-02, -1.5921e-02,  1.3499e-03,  1.7122e-02,\n",
       "         -3.7580e-03,  6.2364e-03,  6.0155e-03,  5.8198e-03, -1.2182e-02,\n",
       "         -7.2245e-04, -9.3548e-03, -3.3590e-03, -5.3551e-06,  1.5731e-02,\n",
       "         -9.9902e-03,  7.7687e-03, -7.1216e-03, -8.2992e-03,  1.2499e-02,\n",
       "         -1.1613e-02,  1.3155e-02,  1.4526e-03,  6.5583e-03, -7.4476e-03,\n",
       "         -1.3927e-02,  6.9591e-03, -1.2890e-02, -5.9258e-04,  4.0786e-03,\n",
       "          1.3889e-03,  1.8896e-02,  2.1990e-04,  7.5760e-03, -7.7886e-03,\n",
       "          6.5767e-03,  7.3358e-03, -1.8483e-02,  6.4378e-03,  7.6060e-03,\n",
       "          9.3186e-03,  6.1644e-03, -1.7286e-02, -1.2642e-02, -1.2544e-02,\n",
       "          1.0027e-02, -1.3544e-02,  1.0997e-02,  2.7804e-03,  1.5869e-02,\n",
       "         -1.4509e-02, -1.0705e-02, -1.3181e-02, -1.3280e-02,  2.5908e-04,\n",
       "         -9.2010e-03,  7.6362e-03, -1.2751e-03, -1.0502e-02,  1.0269e-02,\n",
       "         -4.3890e-03, -2.0168e-02,  1.2679e-02, -1.1686e-02, -1.4656e-02,\n",
       "          1.0035e-04,  1.6264e-02,  9.2834e-03,  1.0942e-02, -1.7121e-03,\n",
       "          8.8143e-03,  1.1118e-02,  1.1003e-02,  1.6286e-02, -4.6027e-03,\n",
       "          8.7585e-03,  1.1505e-02,  4.6196e-03,  5.9966e-03,  9.2445e-03,\n",
       "          6.8237e-03,  6.1334e-03, -3.1609e-03, -3.4975e-04,  1.8026e-02,\n",
       "         -7.3393e-03, -7.3727e-03,  1.0817e-02,  2.2731e-03,  7.2564e-03,\n",
       "          8.5598e-03,  9.1398e-03,  3.7826e-03,  2.9720e-03,  3.7139e-04,\n",
       "         -1.8578e-02, -7.7506e-03, -2.5699e-03,  2.6631e-04, -1.7437e-02,\n",
       "          1.1064e-02, -6.1006e-03, -8.0930e-03,  1.2886e-02, -7.4989e-03,\n",
       "          1.3858e-02, -6.8224e-03,  1.5205e-02,  5.9840e-03,  1.4989e-02,\n",
       "         -3.5836e-03, -5.5969e-03,  2.0692e-02,  1.3623e-02,  1.4310e-02,\n",
       "         -1.3580e-02,  1.2228e-02, -9.8579e-03, -1.3753e-03, -2.3628e-04,\n",
       "          1.1867e-02,  1.1630e-02,  1.4758e-03,  9.6937e-03, -5.9235e-03,\n",
       "          8.9023e-03, -7.4298e-04,  1.5992e-02, -1.6005e-02, -1.4387e-03,\n",
       "         -9.0189e-03, -1.1889e-02,  1.0199e-03,  4.8557e-03,  8.8937e-03,\n",
       "         -8.9012e-03,  5.9866e-03, -7.8242e-03, -8.3487e-03,  4.0992e-03,\n",
       "         -7.3755e-03,  3.5808e-03,  1.1915e-02,  7.0123e-03, -1.8068e-03,\n",
       "         -4.0575e-03,  9.6117e-03, -7.2892e-03,  3.3903e-03,  4.6641e-03,\n",
       "         -1.0961e-02, -1.5412e-02,  8.0085e-03, -1.1173e-02, -3.1892e-03,\n",
       "         -2.7027e-03, -7.4610e-03,  1.1784e-02, -2.2016e-02,  1.7182e-02,\n",
       "          4.1063e-03,  1.0903e-02, -1.3517e-02,  3.7559e-03, -7.5322e-03,\n",
       "         -2.7035e-03,  2.7193e-03,  7.8924e-03,  5.3290e-03,  8.6854e-03,\n",
       "          9.4936e-03,  6.0632e-03, -5.4029e-03, -1.1264e-02,  2.1802e-03,\n",
       "         -7.0990e-03, -7.4815e-03, -1.4272e-02,  1.4155e-02, -1.2203e-02,\n",
       "          2.9515e-03, -1.6429e-02, -1.4726e-02, -1.1025e-02, -1.0271e-02,\n",
       "         -1.7238e-03,  1.1463e-02,  1.0172e-02,  1.1790e-03, -1.9135e-02,\n",
       "         -3.3716e-03,  8.0474e-03, -1.0196e-02,  2.3394e-03,  1.1120e-02,\n",
       "         -6.0843e-03, -1.6538e-02,  1.2374e-02,  3.5603e-03, -4.7551e-03,\n",
       "         -1.4278e-02, -1.8258e-02, -1.7935e-02,  2.4911e-03,  7.6266e-03,\n",
       "          1.6503e-03,  2.9518e-03, -4.1454e-03,  1.5533e-02,  1.4671e-03,\n",
       "         -6.7815e-03, -1.5265e-02,  1.2743e-02,  1.1056e-02,  7.3034e-03,\n",
       "         -1.0747e-02,  7.3510e-03, -9.2118e-03,  3.9315e-03, -3.4607e-03,\n",
       "          1.8995e-03,  2.6601e-03,  6.2334e-03,  1.0556e-02, -1.1097e-02,\n",
       "         -5.7767e-03,  8.3940e-03,  5.8810e-03, -1.5084e-02,  1.3021e-02,\n",
       "          1.0890e-02,  1.2524e-02,  3.2802e-03,  1.0468e-03,  7.9533e-03,\n",
       "         -2.0468e-03,  1.1805e-02, -8.1955e-03,  9.6340e-03,  8.0992e-03,\n",
       "          1.6788e-02, -4.1916e-03, -9.3580e-04,  1.0306e-02, -6.2743e-04,\n",
       "         -1.2948e-02, -3.6065e-03, -9.7105e-03,  1.0677e-02,  1.3225e-02,\n",
       "         -3.5951e-03,  1.7106e-03, -2.8837e-03, -1.5944e-02,  6.6574e-04,\n",
       "         -1.5761e-02,  4.7371e-03,  1.5755e-02,  1.1100e-02, -8.4843e-04,\n",
       "          2.4930e-03, -1.0773e-02,  1.6745e-02,  6.4770e-03, -3.7245e-03,\n",
       "          1.5678e-03,  5.9469e-03,  1.6660e-02, -1.3992e-02,  1.0171e-02,\n",
       "         -2.7090e-03,  1.5485e-02,  1.6358e-02,  5.2748e-03,  9.2867e-03,\n",
       "         -1.0336e-02, -1.4175e-02, -3.6390e-03,  1.8124e-02, -3.2078e-03,\n",
       "         -1.6674e-03,  1.3195e-02, -1.1073e-02, -3.2188e-03, -6.7743e-04,\n",
       "         -6.8837e-03,  3.9161e-03,  4.2998e-03, -1.2865e-03, -8.2615e-03,\n",
       "          4.8885e-03,  6.1738e-03,  1.4147e-02, -1.5395e-03,  2.3027e-03,\n",
       "          8.6654e-03, -1.7990e-02, -1.4321e-02,  1.4528e-02, -1.2870e-02,\n",
       "         -1.5614e-02,  1.0784e-02, -9.7567e-03,  7.1198e-03,  1.7490e-02,\n",
       "         -1.0093e-04,  6.3109e-03,  9.5798e-04, -9.5961e-03,  1.1909e-02,\n",
       "         -1.9779e-02, -2.5642e-03,  6.8273e-03,  7.3345e-03,  8.5270e-03,\n",
       "          4.8834e-03,  9.4849e-05, -1.3550e-02, -6.8737e-03, -3.8117e-03,\n",
       "          5.6727e-03,  5.0468e-03, -9.4423e-03, -1.1429e-02, -3.3497e-03,\n",
       "          6.2446e-03,  1.3537e-02, -6.9928e-04,  6.7249e-03, -5.1043e-03,\n",
       "         -9.7794e-03,  1.5700e-03, -6.2725e-03, -3.6629e-03,  1.0973e-02,\n",
       "         -5.3458e-03, -7.1343e-03, -3.9882e-03, -1.3153e-02, -4.3336e-03,\n",
       "          6.2645e-04, -1.2740e-02, -1.0453e-02, -9.1007e-03, -1.1435e-02,\n",
       "          8.3377e-03, -2.1889e-02,  4.3271e-03, -4.7170e-03,  1.0602e-02,\n",
       "          1.8839e-04, -1.1089e-02,  1.1802e-02,  1.3841e-03,  5.3525e-03,\n",
       "         -3.4065e-03,  1.3340e-02, -1.4215e-02, -1.5879e-02,  3.2804e-03,\n",
       "          1.0920e-02, -1.8300e-02, -1.1679e-02,  2.2757e-02, -6.7626e-03]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image:\", input_image.shape)\n",
    "\n",
    "vgg = VGG16()\n",
    "vgg.__init__()\n",
    "vgg.forward(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. feature map을 추출할 수 있도록 sequential 3개로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_v2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(            \n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(            \n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(9*9*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.layer1(x)\n",
    "        print(\"1st feature map:\", out.shape)\n",
    "        out=self.layer2(out)\n",
    "        print(\"2nd feature map:\", out.shape)\n",
    "        out=self.layer3(out)\n",
    "        print(\"3rd feature map:\", out.shape)\n",
    "#         out=out.view(batch_size, -1)\n",
    "        out=out.view(out.size(0), -1)\n",
    "        out=self.fc_layer(out)\n",
    "        print(\"fc layer shape:\", out.shape)\n",
    "        return out\n",
    "        \n",
    "model = VGG16_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image: torch.Size([1, 3, 300, 300])\n",
      "1st feature map: torch.Size([1, 256, 37, 37])\n",
      "2nd feature map: torch.Size([1, 512, 18, 18])\n",
      "3rd feature map: torch.Size([1, 512, 9, 9])\n",
      "fc layer shape: torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6627e-03, -5.4811e-03,  7.1940e-03,  2.8413e-03,  3.5779e-03,\n",
       "          1.2086e-02, -1.0418e-02,  2.4029e-02,  5.7935e-03,  1.0993e-02,\n",
       "          3.3750e-03,  5.5346e-03, -8.7306e-03, -8.7923e-04, -1.3079e-02,\n",
       "          1.1408e-02,  7.0815e-03,  3.3063e-03, -1.8808e-03,  4.0098e-04,\n",
       "          3.7818e-03,  7.5980e-04,  4.6099e-03, -4.4130e-03, -9.7615e-03,\n",
       "         -2.6397e-03,  2.2819e-03, -1.5259e-02, -2.2287e-03,  1.3305e-03,\n",
       "          7.6571e-03,  1.0547e-02,  1.0508e-03, -1.3387e-02,  1.9768e-02,\n",
       "          1.1887e-02, -3.9477e-03, -6.9013e-03, -1.0001e-03,  1.8150e-02,\n",
       "         -8.6054e-03,  1.0927e-02,  7.4750e-03,  9.7499e-03, -4.0206e-03,\n",
       "         -4.3561e-03, -1.8670e-03,  1.8295e-03, -7.3207e-03, -2.9670e-03,\n",
       "          4.4422e-03, -4.9124e-03,  1.4957e-02, -5.0116e-03,  1.2069e-02,\n",
       "         -4.6408e-03, -1.2279e-02, -3.4053e-03,  9.6161e-03,  1.2978e-02,\n",
       "          6.0171e-03,  4.9745e-03,  4.1003e-03, -1.5143e-02,  4.8552e-03,\n",
       "         -1.2275e-03,  1.1815e-02,  1.4709e-02, -9.5444e-03, -1.7106e-02,\n",
       "         -2.2421e-03,  9.2703e-05,  1.2384e-02,  2.8150e-03,  7.4942e-03,\n",
       "         -4.0212e-03,  2.5488e-03, -3.4082e-03,  1.0657e-02,  1.4284e-02,\n",
       "          3.3492e-03, -4.9656e-03,  2.2917e-02, -8.4125e-03, -4.9140e-03,\n",
       "          2.8414e-04, -9.5653e-03,  1.0108e-02,  9.6165e-03,  1.4132e-02,\n",
       "          8.9793e-03,  4.5838e-03,  6.4050e-03,  1.2514e-02,  1.0897e-02,\n",
       "          8.9200e-04, -1.6242e-02, -6.9832e-03,  5.7330e-03, -5.2640e-03,\n",
       "         -1.2078e-02, -2.8292e-03, -1.5573e-02,  1.0236e-02, -1.2138e-02,\n",
       "         -8.7806e-03,  4.6415e-03, -1.6202e-02,  9.4935e-03, -1.2500e-02,\n",
       "          1.0828e-02,  1.2040e-02, -1.4525e-02, -3.4617e-03,  2.4764e-03,\n",
       "         -8.4179e-03,  1.0193e-02,  2.0969e-04,  1.2696e-02, -1.1247e-02,\n",
       "          1.4289e-02, -2.1987e-02, -1.5276e-02, -6.5545e-04, -4.8766e-03,\n",
       "          1.4681e-02,  1.9443e-02,  1.3785e-03, -1.7543e-02,  1.0836e-02,\n",
       "          3.9935e-03, -1.2192e-03, -8.2273e-03,  2.6057e-03,  3.5421e-03,\n",
       "          3.0827e-03, -1.3625e-02,  7.6229e-03, -2.0547e-03,  9.7590e-03,\n",
       "         -7.4800e-03,  7.3221e-03, -8.2276e-03, -7.5006e-03,  5.6557e-03,\n",
       "         -5.7717e-03,  1.1051e-02, -1.8859e-02,  3.8843e-03,  1.7465e-03,\n",
       "          1.0846e-02, -4.8358e-03,  8.1986e-03,  6.1561e-04,  1.1899e-02,\n",
       "          5.8042e-03,  9.8495e-03,  4.4352e-03, -8.5745e-04,  1.0517e-02,\n",
       "          4.2323e-03,  6.4556e-03,  6.1370e-03, -5.8809e-03, -8.9963e-03,\n",
       "          9.8873e-03, -1.7331e-03,  6.2954e-03, -2.0493e-03, -1.8892e-02,\n",
       "         -6.1904e-04, -1.2038e-02,  9.0967e-03,  1.1661e-02,  1.1833e-02,\n",
       "         -1.5613e-03,  4.1914e-03, -6.9798e-04, -1.1866e-03,  1.4729e-02,\n",
       "         -1.8445e-02,  1.3150e-02, -3.3549e-05, -6.1863e-04,  2.0107e-02,\n",
       "          1.5646e-02, -9.3266e-03, -3.3058e-03, -1.2447e-02,  6.8060e-04,\n",
       "         -7.3526e-03,  4.6711e-03, -1.8368e-02,  1.0627e-02, -1.4411e-02,\n",
       "         -1.6244e-02, -1.3413e-03, -5.0949e-03,  5.1821e-03, -7.1097e-03,\n",
       "         -1.4491e-03, -2.2282e-03,  8.7072e-03, -3.9756e-03,  1.2002e-02,\n",
       "          6.0356e-03, -1.1921e-02,  1.4451e-02, -1.6139e-02,  1.2675e-02,\n",
       "         -2.0593e-03,  7.9949e-03, -1.4570e-02,  6.5818e-03, -3.6176e-03,\n",
       "         -2.4814e-03,  6.9404e-03, -1.2520e-02,  8.6933e-03, -2.4541e-03,\n",
       "         -1.2806e-02, -1.2363e-02, -4.6156e-03, -4.6771e-03,  4.1753e-03,\n",
       "          1.0398e-02,  6.2658e-03,  8.4788e-03,  1.0934e-02, -1.2950e-02,\n",
       "         -1.2877e-03, -1.1608e-02,  1.8204e-02,  1.1401e-02, -8.7301e-03,\n",
       "         -3.4306e-04, -1.4970e-02, -3.9072e-03, -8.9765e-03, -9.7187e-03,\n",
       "          1.0534e-02, -9.1870e-03, -1.7243e-03, -1.1625e-02, -2.2214e-02,\n",
       "          5.6693e-05, -9.0557e-03,  1.9367e-03, -1.4082e-02,  1.2575e-03,\n",
       "         -1.5272e-02, -1.3085e-02, -5.1568e-03,  1.8291e-03,  3.4603e-03,\n",
       "         -9.6277e-03,  9.7794e-03,  7.4292e-03, -3.4994e-03,  4.2327e-03,\n",
       "         -9.2062e-03,  1.4514e-02, -9.0742e-03,  1.0966e-02,  1.0131e-02,\n",
       "         -6.0807e-03, -6.3429e-03, -6.0745e-03, -4.6537e-03,  1.4603e-02,\n",
       "         -1.2579e-02, -9.2546e-03, -2.9101e-03,  2.3867e-03, -1.8820e-03,\n",
       "          4.3344e-03, -6.9658e-03,  1.3779e-02,  4.9033e-03,  1.2816e-03,\n",
       "         -1.2608e-02, -6.0407e-03, -1.3326e-02,  1.5287e-02,  4.8038e-03,\n",
       "          1.1686e-02, -1.6169e-02,  1.2968e-02,  1.5812e-02,  1.5396e-02,\n",
       "          4.9861e-03, -1.4519e-02, -5.0628e-03, -2.5840e-03, -2.0093e-04,\n",
       "         -1.1553e-02,  8.2123e-03,  9.8385e-03,  2.4445e-03, -8.7901e-04,\n",
       "          3.6005e-03,  5.4704e-03,  8.3846e-03, -1.2154e-02,  1.4875e-02,\n",
       "          1.3986e-03,  7.9674e-03, -3.4484e-03, -1.6863e-02,  8.8054e-04,\n",
       "          2.8022e-03,  5.4282e-03, -6.1770e-03,  1.9509e-02,  4.8181e-03,\n",
       "         -1.4830e-02, -8.5544e-03, -5.3553e-03, -1.2954e-02,  1.0420e-02,\n",
       "          5.9861e-03,  1.1013e-02, -4.2574e-03, -1.2090e-02, -5.5118e-03,\n",
       "          1.7343e-02,  5.9283e-03, -2.0566e-03, -1.1634e-02,  1.6996e-03,\n",
       "         -3.9672e-03,  1.4848e-03, -1.5878e-02,  4.9841e-03, -5.5688e-04,\n",
       "          1.5759e-03, -4.1814e-03, -3.4926e-03,  8.8379e-03, -2.5046e-04,\n",
       "          8.7095e-03, -5.9487e-03,  1.1410e-02, -1.2649e-02,  1.2507e-02,\n",
       "          1.0431e-02, -1.4715e-02, -3.1933e-03,  9.8057e-03,  1.4442e-02,\n",
       "         -5.7576e-03,  3.1697e-03, -1.2479e-02, -5.1340e-04, -4.5014e-03,\n",
       "          1.1510e-02,  4.0895e-03, -1.2212e-03,  8.6717e-03, -6.2240e-03,\n",
       "         -1.2721e-03,  5.4634e-03, -1.6520e-02, -5.9253e-03,  1.8227e-02,\n",
       "          5.3209e-03,  1.1811e-02, -1.3276e-02,  4.0461e-03, -5.5852e-03,\n",
       "          2.1769e-02, -6.3753e-03,  2.1166e-03, -6.9000e-03, -1.3469e-02,\n",
       "          1.2281e-02, -1.7234e-02, -6.9643e-03, -9.4377e-03, -1.1061e-02,\n",
       "         -9.9441e-04, -3.1686e-03, -7.2093e-03, -8.6628e-03, -9.4313e-03,\n",
       "         -1.3511e-02, -1.6548e-03, -3.7335e-05,  1.0587e-02,  1.0933e-02,\n",
       "          6.3507e-04, -1.9901e-02, -4.8830e-03, -3.1582e-03, -1.2026e-03,\n",
       "         -5.5481e-03,  9.3665e-03,  9.9270e-03,  9.0464e-03,  1.2534e-02,\n",
       "         -7.7860e-03,  9.7895e-03, -2.3148e-03, -1.4095e-02,  3.2823e-03,\n",
       "         -4.2469e-03,  2.1778e-03,  7.7691e-03,  8.3602e-03, -7.8500e-03,\n",
       "          1.4760e-02,  4.2905e-03,  9.8937e-03,  3.6250e-03,  3.2724e-04,\n",
       "          4.6178e-03, -9.4232e-03,  1.4136e-02,  8.2277e-03,  9.5398e-03,\n",
       "         -2.1060e-02, -1.4389e-02, -1.1328e-02,  7.1373e-03, -4.9734e-03,\n",
       "          6.6172e-04,  1.0002e-02,  1.2896e-02, -4.0276e-03, -6.5320e-03,\n",
       "          1.1133e-02, -1.1336e-03, -1.0994e-02, -1.2137e-02, -7.0901e-03,\n",
       "          1.7795e-02,  1.2514e-02,  1.3628e-02,  4.2759e-03,  1.3820e-02,\n",
       "          9.5918e-03, -1.3547e-02, -3.5412e-03, -8.5843e-03,  3.1734e-03,\n",
       "          1.1414e-02, -2.9532e-03, -1.1487e-02,  7.7596e-03, -1.8648e-02,\n",
       "         -1.7354e-02,  8.2607e-03, -8.7456e-03,  2.5041e-03, -7.3882e-03,\n",
       "          1.0927e-02,  2.0482e-02, -7.6514e-04, -1.6257e-02,  6.4428e-03,\n",
       "          1.6617e-02,  1.2016e-02,  9.8173e-04, -5.2572e-03,  2.3602e-03,\n",
       "         -9.0330e-03, -1.3843e-02,  5.5001e-03, -1.0209e-02, -2.4358e-03,\n",
       "         -8.8524e-03, -1.6979e-04,  1.4435e-02, -1.6577e-02,  1.7793e-02,\n",
       "          5.7865e-03, -1.2581e-02, -1.1019e-02, -1.8335e-04,  9.4592e-03,\n",
       "         -1.5563e-02, -9.2428e-03, -2.4533e-03, -4.9645e-04,  1.2937e-02,\n",
       "          7.1866e-03,  1.5671e-02,  1.0458e-02, -1.9396e-02, -1.4659e-02,\n",
       "         -1.2369e-02,  4.7377e-03, -1.3317e-02,  1.4962e-02, -7.7392e-04,\n",
       "         -1.6732e-03, -3.5725e-03, -4.6595e-03, -5.1598e-03,  1.3917e-02,\n",
       "         -1.3966e-02,  1.7642e-02, -7.7038e-03,  1.0994e-03,  9.7044e-03,\n",
       "          1.3189e-02, -1.0399e-02,  7.8789e-03, -1.9736e-03, -1.0263e-02,\n",
       "         -1.2673e-02,  1.3655e-02, -4.3886e-05,  7.6022e-03,  1.3008e-02,\n",
       "          2.1478e-02,  1.2651e-02, -8.0174e-03,  7.3870e-03,  2.3270e-04,\n",
       "         -4.9458e-03, -9.3174e-03,  3.3104e-03, -4.7059e-03, -3.0289e-03,\n",
       "          9.0131e-03, -1.0946e-02, -1.0611e-02,  1.0429e-03,  7.3754e-03,\n",
       "         -3.7676e-03, -1.9673e-02, -4.1407e-05,  6.5972e-03, -8.0501e-03,\n",
       "         -1.6415e-03, -4.8643e-03,  7.6928e-03,  1.1028e-02, -7.0078e-03,\n",
       "          8.2398e-03, -4.4702e-03,  3.7006e-03,  6.7963e-03, -1.2729e-02,\n",
       "          1.1361e-02,  1.2624e-02,  1.2290e-02, -1.0024e-02, -2.0961e-04,\n",
       "          6.3957e-03, -8.3590e-03, -1.3781e-02,  5.8763e-03,  1.8508e-02,\n",
       "          1.5644e-03,  4.2840e-03,  9.1838e-03,  1.3105e-02, -4.2629e-03,\n",
       "          1.7179e-02, -1.0703e-02,  7.7326e-03,  1.6629e-02, -1.5304e-03,\n",
       "         -4.9369e-03,  1.5578e-02, -1.6517e-02,  5.7003e-03,  2.8312e-03,\n",
       "          1.0833e-02,  1.2833e-02,  2.3303e-03,  7.6012e-03,  9.4006e-03,\n",
       "         -3.6722e-03, -1.4240e-02, -2.7141e-03,  1.0165e-02, -1.2874e-02,\n",
       "          1.2128e-02,  1.0247e-02, -1.1471e-02,  1.1556e-02,  1.5421e-02,\n",
       "          1.1410e-02,  1.1535e-02, -6.0740e-03,  3.2640e-03,  2.1583e-02,\n",
       "         -1.9914e-02,  1.0438e-02, -1.1733e-02, -1.3873e-02,  1.4687e-02,\n",
       "          6.3572e-03,  1.7177e-04,  1.3770e-03,  1.4164e-02,  3.2800e-03,\n",
       "         -7.0358e-03, -2.0488e-03, -3.0089e-04,  8.7452e-03,  1.3557e-02,\n",
       "         -5.6229e-03,  9.9140e-03,  9.9808e-03, -1.7727e-02, -1.8537e-02,\n",
       "          1.9472e-02, -9.8436e-03,  2.1990e-03,  1.5598e-02,  6.9358e-03,\n",
       "          1.4882e-02, -8.7182e-03, -9.3433e-03, -8.0185e-03,  1.1705e-03,\n",
       "         -7.1481e-03, -8.9714e-03,  1.3893e-02,  8.6557e-03, -1.1880e-02,\n",
       "         -4.9689e-03, -1.8454e-03,  1.8076e-02,  1.2228e-02, -7.7241e-03,\n",
       "          4.6519e-03, -8.5650e-03, -8.5823e-03,  1.5561e-02,  7.7892e-03,\n",
       "         -7.8440e-03, -9.9142e-03,  1.6245e-02,  2.0642e-02, -4.7733e-03,\n",
       "         -5.9714e-03, -1.4572e-02,  4.4296e-03, -7.2234e-03, -2.9128e-03,\n",
       "         -9.0342e-03, -5.8788e-03, -5.2460e-03, -1.4595e-02, -1.3297e-02,\n",
       "          1.6958e-02, -1.3150e-02,  1.9805e-02,  3.3214e-03, -3.8476e-03,\n",
       "          3.8450e-03,  6.9339e-03, -3.7879e-03, -5.2739e-03,  4.1833e-03,\n",
       "          1.1174e-02, -8.2210e-03,  1.0094e-02, -1.2170e-02,  5.3359e-03,\n",
       "          1.2379e-02, -3.0868e-03, -8.2497e-03,  5.9158e-03, -1.0427e-02,\n",
       "          1.2286e-02, -1.5175e-02,  1.7333e-02, -2.2887e-02, -3.8797e-03,\n",
       "         -2.0536e-02, -1.9053e-02,  7.3080e-03, -1.3102e-03,  1.6295e-03,\n",
       "          3.6164e-03,  1.3205e-03,  4.0316e-03,  1.2092e-02, -1.3430e-02,\n",
       "         -1.7484e-02,  6.9260e-03, -7.1026e-03,  7.5427e-03,  6.2308e-03,\n",
       "          3.4998e-03,  5.7306e-03,  1.1728e-02,  4.3391e-03, -1.3700e-02,\n",
       "          1.5223e-03,  2.8855e-03,  5.1034e-03,  1.8225e-02,  8.4651e-03,\n",
       "         -5.9364e-04, -7.4391e-03, -8.4273e-03,  3.8508e-03, -1.8403e-03,\n",
       "          8.1080e-03, -1.5360e-02, -1.1070e-02, -9.1457e-03,  5.9682e-03,\n",
       "         -5.1636e-03, -6.0431e-03, -6.2096e-03, -5.5092e-03,  5.6533e-03,\n",
       "          9.8665e-03, -1.3407e-02, -9.3002e-03,  5.9676e-03, -8.6973e-03,\n",
       "         -6.8501e-03,  5.5307e-03, -1.9544e-03, -8.1342e-03, -2.0940e-02,\n",
       "          3.4940e-03,  1.7023e-02, -7.5940e-03,  1.1873e-02, -1.8618e-02,\n",
       "          1.3218e-02,  2.8580e-04,  4.8863e-03,  9.0595e-03,  2.3650e-03,\n",
       "          4.6166e-03, -1.4076e-02,  4.9325e-03,  1.0419e-02,  8.9055e-03,\n",
       "         -8.2526e-03, -1.0461e-02, -1.7173e-02,  8.0254e-03, -1.3903e-02,\n",
       "         -1.1636e-02, -4.1327e-03,  7.6720e-03, -8.7019e-04, -3.2040e-03,\n",
       "         -1.1869e-02, -1.6184e-02, -1.3565e-02, -7.9791e-03,  6.9414e-03,\n",
       "         -4.9665e-03, -1.5584e-02,  3.2984e-03,  9.8535e-03,  6.3146e-03,\n",
       "         -4.5395e-03, -6.1161e-03,  1.7674e-02, -1.9994e-02,  2.7702e-03,\n",
       "         -1.3997e-02,  4.6219e-03, -9.7892e-03,  6.1008e-03,  3.8267e-03,\n",
       "         -6.5016e-03, -9.4497e-03, -9.3461e-03, -2.0603e-02, -1.2228e-03,\n",
       "          5.1648e-03, -1.0556e-02, -1.2672e-02,  6.7362e-03, -3.7655e-03,\n",
       "          4.8595e-03,  1.1037e-02, -2.4051e-03,  1.4750e-03, -1.0232e-03,\n",
       "         -9.4139e-03,  8.6785e-03,  5.4163e-03, -2.8920e-03,  1.7456e-02,\n",
       "          4.9356e-03,  1.0226e-02,  3.8750e-03,  1.3986e-02, -7.5060e-03,\n",
       "          8.8798e-03, -7.6517e-03,  6.0595e-03,  8.9086e-03, -4.6955e-03,\n",
       "         -1.2832e-02,  4.6999e-03,  1.4747e-02, -6.3018e-03,  5.0307e-03,\n",
       "          3.0688e-03,  2.7994e-03, -7.3050e-03, -7.0427e-03, -4.8809e-03,\n",
       "         -1.0565e-02,  7.3432e-04, -1.0797e-02, -1.0178e-02,  1.1314e-02,\n",
       "          1.2339e-02,  9.3839e-04,  1.1134e-02,  6.7254e-03, -7.6265e-03,\n",
       "         -1.3993e-02, -5.0685e-03,  7.4679e-04,  5.4350e-03,  1.7192e-02,\n",
       "         -1.0771e-02, -9.5697e-03, -4.4900e-04,  1.1054e-02,  1.4934e-02,\n",
       "          1.8166e-02,  4.4355e-03,  1.1457e-02, -3.2065e-03, -8.4732e-04,\n",
       "          3.9072e-03,  1.0107e-02, -9.6163e-03,  5.1123e-03,  2.1945e-02,\n",
       "         -4.1609e-04, -1.0594e-03,  1.0480e-03,  1.9637e-02, -1.9025e-02,\n",
       "         -5.2179e-03, -1.2538e-02,  1.1651e-02,  1.6206e-02, -1.1819e-02,\n",
       "          1.2470e-02, -6.9961e-03,  1.2044e-02,  8.7471e-03,  1.6220e-02,\n",
       "          1.3603e-02, -3.1707e-03,  5.0070e-03,  5.3054e-03,  1.1215e-02,\n",
       "          1.0707e-02, -1.3987e-02,  1.1463e-02, -9.6940e-04, -1.3304e-02,\n",
       "          1.3325e-02,  1.0559e-02,  1.0328e-02,  7.9229e-04, -3.1276e-03,\n",
       "          8.0244e-03,  6.5241e-03,  1.8810e-03,  5.0231e-03, -4.0783e-03,\n",
       "         -6.6827e-03, -4.4132e-04, -9.5263e-03,  1.1439e-02,  7.0771e-03,\n",
       "         -1.2679e-02,  1.1185e-02, -1.5553e-02,  1.0954e-02, -1.7829e-02,\n",
       "          1.0760e-02, -7.7756e-03,  7.5647e-04,  1.3190e-02, -9.5153e-03,\n",
       "          8.7598e-03,  9.0753e-03, -1.4677e-02,  4.1945e-03, -7.4099e-03,\n",
       "         -2.8881e-03, -3.8596e-03, -1.6492e-02, -8.9492e-03,  9.2590e-03,\n",
       "          2.8243e-03,  3.3641e-03, -2.7732e-03, -1.1224e-02, -1.0181e-02,\n",
       "          1.9068e-02,  1.3841e-02, -1.5519e-02, -7.8638e-03,  1.3596e-02,\n",
       "         -8.4289e-03,  1.1192e-02, -3.4567e-03, -4.5408e-03, -9.0035e-03,\n",
       "         -6.7617e-04, -6.8304e-03, -4.2534e-03, -1.9315e-02, -1.4869e-02,\n",
       "         -6.9566e-03,  3.7534e-03, -9.8533e-03,  1.2261e-02,  1.0722e-03,\n",
       "          1.5790e-03,  3.2725e-03, -1.2011e-03, -1.1390e-02, -2.0201e-02,\n",
       "         -1.4872e-02, -2.8485e-04, -1.0473e-02,  1.7141e-02,  1.4463e-02,\n",
       "         -1.1582e-02, -1.0378e-02, -7.7796e-03, -5.4621e-03,  1.2080e-02,\n",
       "          3.0572e-03,  7.0623e-03, -1.2071e-02, -5.5518e-03, -1.1295e-02,\n",
       "          2.9281e-03,  3.1415e-03, -2.9822e-03,  5.6354e-03, -5.7496e-03,\n",
       "          9.5469e-03, -1.8366e-02, -5.3122e-03,  4.7743e-04,  3.6307e-04,\n",
       "         -6.2473e-03, -1.3976e-02,  4.9059e-03,  1.5084e-02,  6.3805e-03,\n",
       "          7.3569e-03, -3.3560e-03, -1.2078e-02, -9.1099e-03, -1.5658e-03,\n",
       "         -4.7049e-03, -9.8703e-03, -2.3301e-03, -1.4235e-02,  2.7844e-03,\n",
       "         -1.7556e-03,  8.3617e-03,  1.3447e-02, -4.0993e-03, -1.2081e-02,\n",
       "         -5.5287e-03, -3.0217e-03,  2.5134e-03,  7.8886e-03, -9.5905e-04,\n",
       "          1.0849e-02,  1.1843e-02,  1.0123e-02, -4.3953e-03, -6.4490e-03,\n",
       "         -1.0537e-02,  5.8319e-03, -3.7569e-03, -1.0766e-03, -7.9484e-03,\n",
       "         -1.3355e-02,  8.1731e-03,  4.2813e-04, -6.9614e-03, -1.2302e-03,\n",
       "         -3.0730e-03,  8.8620e-03,  1.6737e-02, -1.8792e-02, -1.6058e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image:\", input_image.shape)\n",
    "\n",
    "vgg = VGG16_v2()\n",
    "vgg.__init__()\n",
    "vgg.forward(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer 하나씩 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 하나씩 만들기\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(9*9*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "            \n",
    "    def forward(self, x):\n",
    "#         out = F.relu(self.conv2(x)), (2, 2)\n",
    "#         out = F.relu(self.conv4(x)), (2, 2)\n",
    "#         out = F.relu(self.conv6(x)), (2, 2)\n",
    "#         out = F.relu(self.conv9(x)), (2, 2)\n",
    "#         out = F.relu(self.conv12(x)), (2, 2)\n",
    "\n",
    "        x = F.relu(self.pool1(x)), (2, 2)\n",
    "        x = F.relu(self.pool2(x)), (2, 2)\n",
    "        x = F.relu(self.pool3(x)), (2, 2)\n",
    "        x = F.relu(self.pool4(x)), (2, 2)\n",
    "        x = F.relu(self.pool5(x)), (2, 2)\n",
    "        \n",
    "        x=x.view(out.size(0), -1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        print(x.shape)\n",
    "        return x\n",
    "        \n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image:  torch.Size([1, 3, 300, 300])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max_pool2d(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c694c03f0e1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvgg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# out1 = layer1(in_image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(out1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f228376ad369>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    158\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ny_torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 576\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mTypeError\u001b[0m: max_pool2d(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "in_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image: \", in_image.shape)\n",
    "\n",
    "\n",
    "vgg = VGG16()\n",
    "vgg.__init__()\n",
    "vgg.forward(in_image)\n",
    "# out1 = layer1(in_image)\n",
    "# print(out1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잘 모르겠어서 한줄씩 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image:  torch.Size([1, 3, 300, 300])\n",
      "conv1: torch.Size([1, 64, 300, 300])\n",
      "conv2: torch.Size([1, 64, 300, 300])\n",
      "pool1:  torch.Size([1, 64, 150, 150])\n",
      "\n",
      "conv3: torch.Size([1, 128, 150, 150])\n",
      "conv4: torch.Size([1, 128, 150, 150])\n",
      "pool2:  torch.Size([1, 128, 75, 75])\n",
      "\n",
      "conv5: torch.Size([1, 256, 75, 75])\n",
      "conv6: torch.Size([1, 256, 75, 75])\n",
      "pool3:  torch.Size([1, 256, 37, 37])\n",
      "\n",
      "conv7: torch.Size([1, 512, 37, 37])\n",
      "conv8: torch.Size([1, 512, 37, 37])\n",
      "pool4:  torch.Size([1, 512, 18, 18])\n",
      "\n",
      "conv9: torch.Size([1, 512, 18, 18])\n",
      "conv10: torch.Size([1, 512, 18, 18])\n",
      "pool5:  torch.Size([1, 512, 9, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_image = torch.randn(1, 3, 300, 300, dtype=torch.float)\n",
    "print(\"input image: \", in_image.shape)\n",
    "\n",
    "conv1 = nn.Conv2d(3, 64, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "feature_map1 = conv1(in_image)\n",
    "print(\"conv1:\", feature_map1.shape)\n",
    "feature_map1 = conv2(feature_map1)\n",
    "print(\"conv2:\", feature_map1.shape)\n",
    "feature_map2 = pool(feature_map1)\n",
    "print(\"pool1: \", feature_map2.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "conv3 = nn.Conv2d(64, 128, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "feature_map3 = conv3(feature_map2)\n",
    "print(\"conv3:\", feature_map3.shape)\n",
    "feature_map3 = conv4(feature_map3)\n",
    "print(\"conv4:\", feature_map3.shape)\n",
    "feature_map4 = pool(feature_map3)\n",
    "print(\"pool2: \", feature_map4.shape)\n",
    "print(\"\")\n",
    "\n",
    "conv5 = nn.Conv2d(128, 256, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "feature_map5 = conv5(feature_map4)\n",
    "print(\"conv5:\", feature_map5.shape)\n",
    "feature_map5 = conv6(feature_map5)\n",
    "print(\"conv6:\", feature_map5.shape)\n",
    "feature_map6 = pool(feature_map5)\n",
    "print(\"pool3: \", feature_map6.shape)\n",
    "print(\"\")\n",
    "\n",
    "conv7 = nn.Conv2d(256, 512, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "feature_map7 = conv7(feature_map6)\n",
    "print(\"conv7:\", feature_map7.shape)\n",
    "feature_map7 = conv8(feature_map7)\n",
    "print(\"conv8:\", feature_map7.shape)\n",
    "feature_map8 = pool(feature_map7)\n",
    "print(\"pool4: \", feature_map8.shape)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "conv9 = nn.Conv2d(512, 512, 3, padding=1)  # 입력, 출력, 필터크기, \n",
    "conv10 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "feature_map9 = conv9(feature_map8)\n",
    "print(\"conv9:\", feature_map9.shape)\n",
    "feature_map9 = conv10(feature_map9)\n",
    "print(\"conv10:\", feature_map9.shape)\n",
    "feature_map10 = pool(feature_map9)\n",
    "print(\"pool5: \", feature_map10.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ny_torch",
   "language": "python",
   "name": "ny_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
